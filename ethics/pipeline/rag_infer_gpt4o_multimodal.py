import os
import pickle
import faiss
import base64
from sentence_transformers import SentenceTransformer
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_core.documents import Document
from openai import OpenAI
from typing import Dict
from dotenv import load_dotenv

load_dotenv()
client = OpenAI(api_key="OPENAPI_KEY")

# âœ… InMemoryDocstore êµ¬í˜„
class InMemoryDocstore:
    def __init__(self, _dict: Dict[int, Document]):
        self._dict = _dict

    def search(self, search: int) -> Document:
        return self._dict[search]

    def add(self, texts: Dict[int, Document]) -> None:
        self._dict.update(texts)

    def delete(self, ids: list) -> None:
        for i in ids:
            self._dict.pop(i, None)

# âœ… base64 ì¸ì½”ë”© í•¨ìˆ˜
def encode_image_to_base64(image_path: str) -> str:
    with open(image_path, "rb") as image_file:
        base64_image = base64.b64encode(image_file.read()).decode("utf-8")
        return f"data:image/jpeg;base64,{base64_image}"

# âœ… ë¬¸ì„œ ë¡œë”© ë° FAISS ë¡œë“œ
MODEL_NAME = "jhgan/ko-sbert-nli"
embedding = HuggingFaceEmbeddings(model_name=MODEL_NAME)
model = SentenceTransformer(MODEL_NAME)

VECTOR_DIR = "./pipeline/vectorstore_kosbert"
INDEX_PATH = f"{VECTOR_DIR}/index.faiss"
META_PATH = f"{VECTOR_DIR}/questions_metadata.pkl"
index = faiss.read_index(INDEX_PATH)
with open(META_PATH, "rb") as f:
    metadata = pickle.load(f)

docs = []
for m in metadata:
    title = m["explanation_title"].strip()
    question = m["question"].strip()
    if m["context"].startswith("[IMAGE:"):
        context_part = m["context_explanation"].strip()
    else:
        context_part = m["context"].strip()
    full_text = f"{title}\n{context_part}\n{question}"
    docs.append(Document(page_content=full_text, metadata=m))

docstore = InMemoryDocstore({i: doc for i, doc in enumerate(docs)})
index_to_docstore_id = {i: i for i in range(len(docs))}
vectorstore = FAISS(
    embedding_function=embedding,
    index=index,
    docstore=docstore,
    index_to_docstore_id=index_to_docstore_id,
)
retriever = vectorstore.as_retriever(search_kwargs={"k": 5})

# âœ… ë©€í‹°ëª¨ë‹¬ RAG ì§ˆì˜ í•¨ìˆ˜
def multimodal_query_gpt4o(question: str, image_path: str):
    relevant_docs = retriever.invoke(question)
    context = "\n\n".join([f"ë¬¸ì„œ {i+1}:\n{doc.page_content}" for i, doc in enumerate(relevant_docs)])
    base64_image = encode_image_to_base64(image_path)

    messages = [
        {
            "role": "user",
            "content": [
                {"type": "text", "text": f"ë‹¤ìŒì€ ìˆ˜ëŠ¥ ëª¨ì˜ê³ ì‚¬ ë¬¸ì œì…ë‹ˆë‹¤. ë¬¸ì„œë“¤ê³¼ ì´ë¯¸ì§€ë¥¼ ì°¸ê³ í•˜ì—¬, '{question}'\n\në¬¸ì„œë“¤:\n{context}"},
                {
                    "type": "image_url",
                    "image_url": {"url": base64_image},
                },
            ],
        }
    ]

    response = client.chat.completions.create(
        model="gpt-4o",
        messages=messages,
        temperature=0.2,
    )

    return response.choices[0].message.content.strip()

# ì‹¤í–‰
if __name__ == "__main__":
    image_file_path = "./data/test_image/test6.png"  # ì—…ë¡œë“œí•œ ì´ë¯¸ì§€ íŒŒì¼
    question_text = "ì´ ë¬¸ì œì˜ ì •ë‹µê³¼ í•´ì„¤ ì•Œë ¤ì¤˜."
    answer = multimodal_query_gpt4o(question_text, image_file_path)
    print("\nğŸ§  GPT-4oì˜ ë©€í‹°ëª¨ë‹¬ ì‘ë‹µ:\n")
    print(answer)
