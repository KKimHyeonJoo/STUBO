import pickle, faiss, os
from sentence_transformers import SentenceTransformer
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.retrievers import BM25Retriever
from langchain_core.documents import Document
from typing import Dict

class InMemoryDocstore:
    def __init__(self, _dict: Dict[int, Document]):
        self._dict = _dict

    def search(self, search: int) -> Document:
        return self._dict[search]

    def add(self, texts: Dict[int, Document]) -> None:
        self._dict.update(texts)

    def delete(self, ids: list) -> None:
        for i in ids:
            self._dict.pop(i, None)

# ì„¤ì •
VECTOR_DIR = "./pipeline/vectorstore_kosbert"
INDEX_PATH = f"{VECTOR_DIR}/index.faiss"
META_PATH = f"{VECTOR_DIR}/questions_metadata.pkl"
MODEL_NAME = "jhgan/ko-sbert-nli"
TOP_K = 5
QUERIES = [
    "ì„ê°€ëª¨ë‹ˆê°€ ë§í•œ ê³ í†µì˜ ì›ì¸ì€ ë¬´ì—‡ì¸ê°€?",
    "ê³µìì™€ ë§¹ìì˜ ì¸ê°„ê´€ì€ ì–´ë–»ê²Œ ë‹¤ë¥¸ê°€?",
    "ì‚¬íšŒê³„ì•½ë¡ ì˜ ì£¼ìš” ì‚¬ìƒê°€ ë‘ ëª…ì„ ì„¤ëª…í•˜ë¼"
]

# ëª¨ë¸ ë° ë²¡í„° ë¡œë”©
embedding = HuggingFaceEmbeddings(model_name=MODEL_NAME)
model = SentenceTransformer(MODEL_NAME)
index = faiss.read_index(INDEX_PATH)
with open(META_PATH, "rb") as f:
    metadata = pickle.load(f)

# LangChainìš© ë¬¸ì„œ êµ¬ì„±
docs = []
for m in metadata:
    title = m["explanation_title"].strip()
    question = m["question"].strip()
    if m["context"].startswith("[IMAGE:"):
        context_part = m["context_explanation"].strip()
    else:
        context_part = m["context"].strip()
    full_text = f"{title}\n{context_part}\n{question}"
    docs.append(Document(page_content=full_text, metadata=m))

# FAISSìš© docstore ë° ë§¤í•‘ ì„¤ì •
docstore = InMemoryDocstore({i: doc for i, doc in enumerate(docs)})
index_to_docstore_id = {i: i for i in range(len(docs))}

# Vectorstore ìƒì„± (ìµœì‹  langchain-compatible ë°©ì‹)
vectorstore = FAISS(
    embedding_function=embedding,
    index=index,
    docstore=docstore,
    index_to_docstore_id=index_to_docstore_id,
)

# Retriever 2ì¢… êµ¬ì„±
retrievers = {
    "dense_k5": vectorstore.as_retriever(search_kwargs={"k": TOP_K}),
    "bm25_k5": BM25Retriever.from_documents(docs)
}

# ê²°ê³¼ ì €ì¥
os.makedirs("pipeline", exist_ok=True)
with open("pipeline/retriever_results.txt", "w", encoding="utf-8") as f:
    for name, retriever in retrievers.items():
        f.write(f"\n=== ğŸ” Retriever: {name} ===\n")
        for query in QUERIES:
            f.write(f"\n[Query] {query}\n")
            results = retriever.get_relevant_documents(query)
            for i, doc in enumerate(results):
                content = doc.page_content[:150].replace('\n', ' ')
                f.write(f" ({i+1}) {content}...\n")
