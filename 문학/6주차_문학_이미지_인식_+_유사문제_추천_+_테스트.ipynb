{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiOQqoaFr9xC"
      },
      "source": [
        "# **0. íŒ¨í‚¤ì§€ ì„¤ì¹˜**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_3-jzXMaaaP",
        "outputId": "08a24016-e90e-4d07-f289-f6d2ef9c70c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.26)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: langchain-core in /usr/local/lib/python3.11/dist-packages (0.3.68)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.4.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (8.5.0)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (4.14.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.7.9)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.1)\n",
            "Downloading langchain_community-0.3.27-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\n",
            "Downloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-community\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.1 langchain-community-0.3.27 marshmallow-3.26.1 mypy-extensions-1.1.0 pydantic-settings-2.10.1 python-dotenv-1.1.1 typing-inspect-0.9.0\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.26)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.93.3)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.68)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.4.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.14.1)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.7.9)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (1.33)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.4.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain) (3.0.0)\n",
            "Downloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl (31.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.11.0\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.93.3)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.14.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.7.9)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U langchain langchain-community langchain-core\n",
        "!pip install langchain openai faiss-cpu\n",
        "!pip install openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BC67ybd_0zG"
      },
      "source": [
        "# **0. êµ¬ê¸€ ë“œë¼ì´ë¸Œ ë§ˆìš´íŠ¸ & api key**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKc9Ou1b1KcO",
        "outputId": "bc6b9a1b-8301-4d85-e31c-eba6ba34a5a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S-rAxqSqaLz1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "#  API í‚¤ ì„¤ì •\n",
        "OPENAI_API_KEY = \"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\"  # ì—¬ê¸°ì—ë§Œ ì…ë ¥í•˜ë©´ ì•„ë˜ì—ì„œ ìë™ ì‚¬ìš©ë¨\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "client = openai.OpenAI(api_key=OPENAI_API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. ê¸°ì¡´ Retriever ë¡œë“œ (3ì£¼ì°¨-2ì—ì„œ ìƒì„±í–ˆì—ˆìŒ)**"
      ],
      "metadata": {
        "id": "WAiFFmILvFFR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.embeddings import OpenAIEmbeddings\n",
        "\n",
        "# ğŸ”¹ ê²½ë¡œ ì„¤ì •\n",
        "index_path = \"/content/drive/MyDrive/Colab Notebooks/TAVE í”„ë¡œì íŠ¸_STUBO/ë¬¸í•™/data/faiss_index_ìœ ì‚¬ë¬¸ì œ\"\n",
        "embedding_model = OpenAIEmbeddings()\n",
        "\n",
        "# ğŸ”¹ ê¸°ì¡´ FAISS ì¸ë±ìŠ¤ ë¡œë“œ\n",
        "print(\"ğŸ“‚ ê¸°ì¡´ FAISS ì¸ë±ìŠ¤ ë¡œë“œ ì¤‘...\")\n",
        "retriever_store = FAISS.load_local(index_path, embedding_model, allow_dangerous_deserialization=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PbmfywF6qg6I",
        "outputId": "1c635db0-8a6a-4d15-fd9f-2c50c57789f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“‚ ê¸°ì¡´ FAISS ì¸ë±ìŠ¤ ë¡œë“œ ì¤‘...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. ì‚¬ìš©ì ì§ˆë¬¸ tag ìƒì„± í”„ë¡¬í”„íŠ¸**"
      ],
      "metadata": {
        "id": "wDHgeg9sraka"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ğŸ”¹ ì‚¬ìš©ì ì§ˆë¬¸ â†’ GPT íƒœê¹…\n",
        "def get_tags_from_gpt(query):\n",
        "    prompt = f\"\"\"\n",
        "            ë‹¤ìŒ ë¬¸í•™ ì§€ë¬¸ê³¼ ë¬¸ì œë¥¼ ì½ê³  ì•„ë˜ í•­ëª©ì„ JSON í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.\n",
        "\n",
        "            ğŸ“š ì…ë ¥ ì •ë³´\n",
        "            (ì§€ë¬¸&ë¬¸ì œ)\n",
        "            {query}\n",
        "\n",
        "            ğŸ§© ë³µí•©/ë‹¨ì¼ íŒë‹¨ ê¸°ì¤€:\n",
        "            - ì§€ë¬¸ì´ 2ê°œ ì´ìƒì´ë©´ ë°˜ë“œì‹œ \"ë³µí•©\"ìœ¼ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”.\n",
        "            - ë¬¸ì œì—ì„œ 'ê³µí†µì ', 'ë¹„êµ', 'ë‹¤ìŒ ê¸€ë“¤', '(ê°€)ì™€ (ë‚˜)'ë¼ëŠ” í‘œí˜„ì´ ë“±ì¥í•˜ë©´ ë°˜ë“œì‹œ \"ë³µí•©\"ìœ¼ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”.\n",
        "            - ìœ„ ì¡°ê±´ ì¤‘ í•˜ë‚˜ë¼ë„ ì¶©ì¡±í•˜ë©´ ë°˜ë“œì‹œ \"ë³µí•©\"ìœ¼ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”.\n",
        "            - ì§€ë¬¸ì´ 1ê°œì´ê±°ë‚˜ ì§€ë¬¸ ì—¬ëŸ¬ê°œ ì¤‘ì—ì„œ ë¬¸ì œì—ì„œ í•œ íŠ¹ì • ì§€ë¬¸ë§Œ ë¬»ëŠ” ê²½ìš°ì—ë§Œ \"ë‹¨ì¼\"ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”.\n",
        "            âš ï¸ ë³µí•©/ë‹¨ì¼ ë¶„ë¥˜ëŠ” ì ˆëŒ€ í‹€ë¦¬ë©´ ì•ˆ ë©ë‹ˆë‹¤. ë°˜ë“œì‹œ ì£¼ì˜í•˜ì„¸ìš”.\n",
        "\n",
        "            ğŸ§  ë¬¸ì œ ìœ í˜• ë¶„ë¥˜ ê¸°ì¤€ (ì§€ë¬¸ ì¥ë¥´ë³„ë¡œ ì•„ë˜ ì¤‘ í•˜ë‚˜ ì„ íƒ):\n",
        "            - í˜„ëŒ€ì‹œ:\n",
        "              - ë‚´ìš© ì´í•´: ì‹œì  ìƒí™©, ì£¼ì œ, ì •ì„œ ë° íƒœë„ ë“± íŒŒì•…\n",
        "              - ì •ì„œ ë° íƒœë„ íŒŒì•…: í™”ìì˜ ì‹¬ë¦¬ì™€ íƒœë„ íë¦„ ì´í•´\n",
        "              - í‘œí˜„ ë°©ì‹ ë¶„ì„: ë¹„ìœ , ìƒì§•, ë°˜ë³µ, ì„¤ì˜ ë“±\n",
        "              - ì‹œì–´ í•´ì„: ê°œë³„ ì‹œì–´ ë˜ëŠ” í‘œí˜„ì˜ ìƒì§•/ì¤‘ì˜ì  ì˜ë¯¸ í•´ì„\n",
        "              - ìƒì§•/ë¹„ìœ  í•´ì„: ì¤‘ì‹¬ ì´ë¯¸ì§€ë‚˜ ìƒì§• êµ¬ì¡° í•´ì„\n",
        "\n",
        "            - ê³ ì „ ì‹œê°€:\n",
        "              - ë‚´ìš© ì´í•´: ì „ì²´ ì˜ë¯¸, ì‘í’ˆ íë¦„, ì •ì„œ ì´í•´\n",
        "              - ì •ì„œ íŒŒì•…: ì„ì— ëŒ€í•œ ë§ˆìŒ, ìì—°/í˜„ì‹¤ ì¸ì‹\n",
        "              - í‘œí˜„ ê¸°ë²• ë¶„ì„: ê³ ì „ì  ìˆ˜ì‚¬ ê¸°ë²• ë¶„ì„ (ì˜íƒ„, ëŒ€ì¡°, ê³¼ì¥ ë“±)\n",
        "              - ì„-í™”ì ê´€ê³„ ì´í•´: êµìˆ /ì„œì • ì‹œê°€ì—ì„œì˜ ê´€ê³„ ë§¥ë½\n",
        "              - ë³€ì‹ /í™˜ìƒ í‘œí˜„ í•´ì„: ì‹ í™”/í™˜ìƒì  ìš”ì†Œ í•´ì„\n",
        "\n",
        "            - í˜„ëŒ€ ì†Œì„¤:\n",
        "              - ì‚¬ê±´ íë¦„ íŒŒì•…: ì¤„ê±°ë¦¬ ë° ì£¼ìš” ì‚¬ê±´ íë¦„ ì´í•´\n",
        "              - ì¸ë¬¼ ì‹¬ë¦¬ ì´í•´: ì¸ë¬¼ì˜ ì„±ê²©, ë‚´ì  ì‹¬ë¦¬, ê´€ê³„ í•´ì„\n",
        "              - ì‹œì  ë° ì„œìˆ  ë°©ì‹ ë¶„ì„: ì„œìˆ ì, ì‹œì , ë¬˜ì‚¬ ë°©ì‹ ë¶„ì„\n",
        "              - ì£¼ì œ/ì‘ê°€ ì˜ë„ íŒŒì•…: ì¤‘ì‹¬ ì£¼ì œ, ì£¼ì œì˜ì‹ ë¶„ì„\n",
        "              - ê³µê°„/ë°°ê²½ ì˜ë¯¸ ë¶„ì„: ë°°ê²½ì´ ê°€ì§€ëŠ” ìƒì§•ì  ì˜ë¯¸ í•´ì„\n",
        "\n",
        "            - ê³ ì „ ì†Œì„¤:\n",
        "              - ë‚´ìš© ì´í•´: ì¤„ê±°ë¦¬, ì‚¬ê±´ êµ¬ì¡° íŒŒì•…\n",
        "              - ì¸ë¬¼ ì‹¬ë¦¬ ë° ìš´ëª… íŒŒì•…: ì£¼ìš” ì¸ë¬¼ì˜ ì„±ê²©ê³¼ ìš´ëª…\n",
        "              - ìƒì§• ì¥ì¹˜ í•´ì„: ê¿ˆ, ì „ê¸°, ìì—° ìš”ì†Œ ë“± ìƒì§• êµ¬ì¡° í•´ì„\n",
        "              - ê¶Œì„ ì§•ì•…ì  ê´€ì  ë¶„ì„: ì¸ê³¼ì  ì„¸ê³„ê´€, ë„ë•ì  êµí›ˆ í•´ì„\n",
        "              - ì„œì‚¬ êµ¬ì¡° ë¶„ì„: ë„ì…-ì „ê°œ-ìœ„ê¸°-ì ˆì •-ê²°ë§ì˜ êµ¬ì¡°\n",
        "\n",
        "            - ê·¹/ìˆ˜í•„:\n",
        "              - ë‚´ìš© ì´í•´: ìƒí™©, ëŒ€ì‚¬, ì‚¬ê±´ì˜ íë¦„ ì´í•´\n",
        "              - í‘œí˜„ íŠ¹ì„± ë¶„ì„: ëŒ€ì‚¬, í•´ì„¤, ì¥ë©´ êµ¬ì„±ì˜ íŠ¹ì§•\n",
        "              - ì„œìˆ ìì˜ ê°œì… íŒŒì•…: ìˆ˜í•„/ê·¹ ì¤‘ ì„œìˆ ìì˜ ìœ„ì¹˜ ë° ì—­í• \n",
        "              - ì£¼ì œ ë° êµí›ˆ ë„ì¶œ: ì¤‘ì‹¬ ì£¼ì œ ë° ì‚¶ì— ì£¼ëŠ” êµí›ˆ í•´ì„\n",
        "\n",
        "            âœ’ï¸ ì§€ë¬¸ ì œëª© ë° ì‘ê°€ ì¶”ì¶œ ê¸°ì¤€:\n",
        "            - ì¼ë°˜ì ìœ¼ë¡œ ê° ì§€ë¬¸ ëì— ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ í‘œê¸°ë¨:\n",
        "              `- ê¹€ìˆ˜ì˜ , ï½¢ê·¸ ë°©ì„ ìƒê°í•˜ë©°ï½£ -`\n",
        "              â†’ ì‘ê°€: \"ê¹€ìˆ˜ì˜\", ì œëª©: \"ê·¸ ë°©ì„ ìƒê°í•˜ë©°\"\n",
        "            - ë³µí•© ì§€ë¬¸ì¼ ê²½ìš°:\n",
        "              - \"ì§€ë¬¸ ì œëª©\": [\"ì œëª©1\", \"ì œëª©2\", ...]\n",
        "              - \"ì§€ë¬¸ ì‘ê°€\": [\"ì‘ê°€1\", \"ì‘ê°€2\", ...]\n",
        "              - ì‘ì ë¯¸ìƒì¼ ê²½ìš° \"ì‘ì ë¯¸ìƒ\"ìœ¼ë¡œ í‘œê¸°\n",
        "              âš ï¸ ì œëª© ë˜ëŠ” ì‘ê°€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ëŠ” ê²½ìš°:\n",
        "              - ë°˜ë“œì‹œ ë‹¤ìŒê³¼ ê°™ì´ ì‘ì„±í•˜ì„¸ìš”.\n",
        "              - ì§€ë¬¸ ì œëª©: \"ì§€ë¬¸ ì œëª© ì—†ìŒ\"\n",
        "              - ì§€ë¬¸ ì‘ê°€: \"ì‘ì ë¯¸ìƒ\"\n",
        "              - ë³µí•© ì§€ë¬¸ì¼ ê²½ìš°: [\"ì§€ë¬¸ ì œëª© ì—†ìŒ\", \"ì§€ë¬¸ ì œëª© ì—†ìŒ\"], [\"ì‘ì ë¯¸ìƒ\", \"ì‘ì ë¯¸ìƒ\"]\n",
        "\n",
        "            ğŸ“Œ ì¶œë ¥ í˜•ì‹ (ëª¨ë‘ í¬í•¨):\n",
        "            - type: ë°˜ë“œì‹œ \"ë¬¸í•™\"\n",
        "            - ì§€ë¬¸ ì œëª©: ë¬¸ìì—´ ë˜ëŠ” ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸\n",
        "            - ì§€ë¬¸ ì¥ë¥´: ë¬¸ìì—´ ë˜ëŠ” ë¦¬ìŠ¤íŠ¸ (ê°ˆë˜ ë³µí•©ì´ë©´ ë¦¬ìŠ¤íŠ¸)\n",
        "            - ì§€ë¬¸ ì‘ê°€: ë¬¸ìì—´ ë˜ëŠ” ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸\n",
        "            - ë³µí•©/ë‹¨ì¼: \"ë³µí•©\" ë˜ëŠ” \"ë‹¨ì¼\"\n",
        "            - ë¬¸ì œ ìœ í˜•: ìœ„ ê¸°ì¤€ ì¤‘ ì ì ˆí•œ ê²ƒ í•˜ë‚˜\n",
        "\n",
        "            ğŸ“Œ ì¶œë ¥ ì˜ˆì‹œ:\n",
        "\n",
        "            [ì§€ë¬¸ 1ê°œ ì˜ˆì‹œ]\n",
        "            {{\n",
        "              \"type\": \"ë¬¸í•™\",\n",
        "              \"ì§€ë¬¸ ì œëª©\": \"ìˆ™í–¥ì „\",\n",
        "              \"ì§€ë¬¸ ì¥ë¥´\": \"ê³ ì „ ì†Œì„¤\",\n",
        "              \"ì§€ë¬¸ ì‘ê°€\": \"ì‘ì ë¯¸ìƒ\",\n",
        "              \"ë³µí•©/ë‹¨ì¼\": \"ë‹¨ì¼\",\n",
        "              \"ë¬¸ì œ ìœ í˜•\": \"ì¸ë¬¼ ì‹¬ë¦¬ ë° ìš´ëª… íŒŒì•…\"\n",
        "            }}\n",
        "\n",
        "            [ì§€ë¬¸ ì—¬ëŸ¬ê°œ ì˜ˆì‹œ]\n",
        "            {{\n",
        "              \"type\": \"ë¬¸í•™\",\n",
        "              \"ì§€ë¬¸ ì œëª©\": [\"ë³„ì‚¬ë¯¸ì¸ê³¡\", \"ì œëª© ì—†ìŒ\", \"ë°±ìì¦ì •ë¶€ì¸ë°•ì”¨ë¬˜ì§€ëª…\"],\n",
        "              \"ì§€ë¬¸ ì¥ë¥´\": [\"ê³ ì „ ì‹œê°€\", \"ê³ ì „ ì‹œê°€\", \"ê³ ì „ ì‚°ë¬¸\"],\n",
        "              \"ì§€ë¬¸ ì‘ê°€\": [\"ê¹€ì¶˜íƒ\", \"ì´ì •ë³´\", \"ë°•ì§€ì›\"],\n",
        "              \"ë³µí•©/ë‹¨ì¼\": \"ë³µí•©\",\n",
        "              \"ë¬¸ì œ ìœ í˜•\": \"ì •ì„œ íŒŒì•…\"\n",
        "            }}\n",
        "        \"\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"ë‹¹ì‹ ì€ ìˆ˜ëŠ¥ êµ­ì–´ ë¬¸í•™ ì „ë¬¸ íƒœê¹… ë„ìš°ë¯¸ì…ë‹ˆë‹¤. âš ï¸ ë°˜ë“œì‹œ ì½”ë“œ ë¸”ë¡ ì—†ì´ ìˆœìˆ˜ JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt},\n",
        "        ],\n",
        "        temperature=0.2,\n",
        "        max_tokens=1000\n",
        "    )\n",
        "\n",
        "    content = response.choices[0].message.content.strip()\n",
        "    # GPTê°€ ì½”ë“œ ë¸”ë¡ìœ¼ë¡œ ê°ìŒ€ ê²½ìš° ì œê±°\n",
        "    content = re.sub(r\"```json\\s*([\\s\\S]+?)\\s*```\", r\"\\1\", content)\n",
        "    content = re.sub(r\"```[\\s\\S]+?```\", \"\", content).strip()\n",
        "\n",
        "    try:\n",
        "        return json.loads(content)\n",
        "    except json.JSONDecodeError:\n",
        "        print(\"âŒ GPT ì‘ë‹µ JSON íŒŒì‹± ì‹¤íŒ¨:\\n\", content)\n",
        "        return None"
      ],
      "metadata": {
        "id": "h5YTLtiWrdOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "import openai\n",
        "\n",
        "# ğŸ”¸ GPT í´ë¼ì´ì–¸íŠ¸ ì„¤ì •\n",
        "client = openai.OpenAI()\n",
        "\n",
        "# ğŸ”¸ ë¬¸ì œ ì½”ë“œ ì¶”ì¶œ í•¨ìˆ˜\n",
        "def extract_question_code(source_str):\n",
        "    if not source_str:\n",
        "        return None\n",
        "    source_str = source_str.replace(\"á„€á…®á†¨á„‹á…¥\", \"êµ­ì–´\")\n",
        "    match = re.search(r'(\\d{4}-(?:\\d{2}|ìˆ˜ëŠ¥)-êµ­ì–´_\\d+)$', source_str)\n",
        "    if match:\n",
        "        return match.group(1)\n",
        "    all_matches = re.findall(r'(\\d{4}-(?:\\d{2}|ìˆ˜ëŠ¥)-êµ­ì–´_\\d+)', source_str)\n",
        "    if all_matches:\n",
        "        return all_matches[-1]\n",
        "    return source_str\n",
        "\n",
        "# ğŸ”¸ ë¬¸ì„œ ë©”íƒ€ë°ì´í„°ì— íƒœê·¸ ë³‘í•©\n",
        "def merge_tags_to_docs(docs, tag_dict):\n",
        "    for doc in docs:\n",
        "        source = doc.metadata.get(\"source\")\n",
        "        code = extract_question_code(source)\n",
        "        if code and code in tag_dict:\n",
        "            doc.metadata[\"question_tags\"] = tag_dict[code]\n",
        "    return docs"
      ],
      "metadata": {
        "id": "W-XTPgI3CkOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ğŸ”¹ íƒœê·¸ ìœ ì‚¬ë„ ì ìˆ˜ ê³„ì‚° (ë¶€ë¶„ ì ìˆ˜ í¬í•¨)\n",
        "def tag_similarity_score(user_tags, doc_tags):\n",
        "    score = 0\n",
        "\n",
        "    # ë¬¸ì œ ìœ í˜• (4ì , ì™„ì „ ì¼ì¹˜ ì‹œ)\n",
        "    if user_tags.get(\"ë¬¸ì œ ìœ í˜•\") == doc_tags.get(\"ë¬¸ì œ ìœ í˜•\"):\n",
        "        score += 4\n",
        "\n",
        "    # ë³µí•©/ë‹¨ì¼ (2ì )\n",
        "    if user_tags.get(\"ë³µí•©/ë‹¨ì¼\") == doc_tags.get(\"ë³µí•©/ë‹¨ì¼\"):\n",
        "        score += 2\n",
        "\n",
        "    # ì§€ë¬¸ ì¥ë¥´ (ë¶€ë¶„ ì ìˆ˜ ë¶€ì—¬)\n",
        "    user_genre = user_tags.get(\"ì§€ë¬¸ ì¥ë¥´\")\n",
        "    doc_genre = doc_tags.get(\"ì§€ë¬¸ ì¥ë¥´\")\n",
        "\n",
        "    if user_genre == doc_genre:\n",
        "        score += 5\n",
        "    else:\n",
        "        # ë¹„ìŠ·í•œ ì¥ë¥´ ë¶€ë¶„ ì ìˆ˜\n",
        "        genre_similarities = [\n",
        "            (\"í˜„ëŒ€ì‹œ\", \"ê³ ì „ ì‹œê°€\"),\n",
        "            (\"ê³ ì „ ì‹œê°€\", \"í˜„ëŒ€ì‹œ\"),\n",
        "            (\"í˜„ëŒ€ ì†Œì„¤\", \"ê³ ì „ ì†Œì„¤\"),\n",
        "            (\"ê³ ì „ ì†Œì„¤\", \"í˜„ëŒ€ ì†Œì„¤\"),\n",
        "        ]\n",
        "        if (user_genre, doc_genre) in genre_similarities or (doc_genre, user_genre) in genre_similarities:\n",
        "            score += 2\n",
        "\n",
        "    # ì§€ë¬¸ ì œëª© (1ì )\n",
        "    user_title = user_tags.get(\"ì§€ë¬¸ ì œëª©\")\n",
        "    doc_title = doc_tags.get(\"ì§€ë¬¸ ì œëª©\")\n",
        "\n",
        "    if isinstance(user_title, list) and isinstance(doc_title, list):\n",
        "        if set(user_title) & set(doc_title):\n",
        "            score += 1\n",
        "    elif isinstance(user_title, list):\n",
        "        if doc_title in user_title:\n",
        "            score += 1\n",
        "    elif isinstance(doc_title, list):\n",
        "        if user_title in doc_title:\n",
        "            score += 1\n",
        "    else:\n",
        "        if user_title == doc_title:\n",
        "            score += 1\n",
        "\n",
        "    # ì§€ë¬¸ ì‘ê°€ (1ì )\n",
        "    user_name = user_tags.get(\"ì§€ë¬¸ ì‘ê°€\")\n",
        "    doc_name = doc_tags.get(\"ì§€ë¬¸ ì‘ê°€\")\n",
        "\n",
        "    if isinstance(user_name, list) and isinstance(doc_name, list):\n",
        "        if set(user_name) & set(doc_name):\n",
        "            score += 1\n",
        "    elif isinstance(user_name, list):\n",
        "        if doc_name in user_name:\n",
        "            score += 1\n",
        "    elif isinstance(doc_name, list):\n",
        "        if user_name in doc_name:\n",
        "            score += 1\n",
        "    else:\n",
        "        if user_name == doc_name:\n",
        "            score += 1\n",
        "\n",
        "    return score"
      ],
      "metadata": {
        "id": "iCIC_wki6aJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ğŸ”¹ ìœ ì‚¬ ë¬¸ì œ ì¶”ì²œ(ì´ë¯¸ì§€X)\n",
        "def show_similar_problems(user_question, retriever, tag_dict, top_k=2):\n",
        "    print(\"\\nğŸ§  [ìœ ì‚¬ ë¬¸ì œ ì¶”ì²œ ì‹œì‘]\")\n",
        "    user_tags = get_tags_from_gpt(user_question)\n",
        "    if user_tags is None:\n",
        "        print(\"âŒ ì‚¬ìš©ì ì§ˆë¬¸ íƒœê¹… ì‹¤íŒ¨\")\n",
        "        return\n",
        "\n",
        "    print(\"\\n=== âœ… ì‚¬ìš©ì ì§ˆë¬¸ íƒœê·¸ ===\")\n",
        "    print(json.dumps(user_tags, ensure_ascii=False, indent=2))\n",
        "\n",
        "    results = retriever.similarity_search_with_score(user_question, k=30)\n",
        "\n",
        "    docs = []\n",
        "    for doc, score in results:\n",
        "        doc.metadata[\"score\"] = score  # â† ìœ ì‚¬ë„ ì ìˆ˜ë¥¼ metadataì— ë„£ì–´ì¤Œ\n",
        "        docs.append(doc)\n",
        "\n",
        "    docs = merge_tags_to_docs(docs, tag_dict)\n",
        "\n",
        "    docs_with_score = []\n",
        "    for doc in docs:\n",
        "        doc_tags = doc.metadata.get(\"question_tags\")\n",
        "        if not doc_tags:\n",
        "            continue\n",
        "        tag_sim = tag_similarity_score(user_tags, doc_tags)\n",
        "        embedding_sim = doc.metadata.get(\"score\", 0)\n",
        "        final_score = round(tag_sim * 0.7 + embedding_sim * 0.3, 4)\n",
        "        docs_with_score.append((doc, doc_tags, tag_sim, embedding_sim, final_score))\n",
        "\n",
        "    if not docs_with_score:\n",
        "        print(\"âŒ íƒœê·¸ ìˆëŠ” ìœ ì‚¬ ë¬¸í•­ ì—†ìŒ\")\n",
        "        return\n",
        "\n",
        "    docs_sorted = sorted(docs_with_score, key=lambda x: x[4], reverse=True)\n",
        "\n",
        "    print(f\"\\n=== ğŸ” ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ìœ ì‚¬í•œ ê¸°ì¶œ ë¬¸ì œ Top {top_k} ===\")\n",
        "    for i, (doc, doc_tags, tag_sim, emb_sim, final_score) in enumerate(docs_sorted[:top_k]):\n",
        "        print(f\"\\n[ìœ ì‚¬ ë¬¸ì œ {i + 1}]\")\n",
        "        print(f\"ğŸ“Œ ì¶œì²˜: {doc.metadata.get('source')}\")\n",
        "        print(f\"ğŸ§® íƒœê·¸ ìœ ì‚¬ë„: {tag_sim:.4f} | ì„ë² ë”© ìœ ì‚¬ë„: {emb_sim:.4f} | ìµœì¢… ì ìˆ˜: {final_score:.4f}\")\n",
        "        print(\"\\nğŸ“– ì§€ë¬¸:\")\n",
        "        print(doc.metadata.get(\"passage\", \"\").strip() or \"(ì§€ë¬¸ ì—†ìŒ)\")\n",
        "        print(\"\\nâ“ ë¬¸ì œ:\")\n",
        "        print(doc.metadata.get(\"question\", \"\").strip() or doc.page_content.strip())\n",
        "        print(\"\\nğŸ·ï¸ íƒœê·¸ ì •ë³´:\")\n",
        "        print(json.dumps(doc_tags, ensure_ascii=False, indent=2))"
      ],
      "metadata": {
        "id": "KqiDuCWy6ddY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. ì„ë² ë”© ìœ ì‚¬ë„ + íƒœê·¸ ê¸°ë°˜ ìœ ì‚¬ë„ + ë¬¸ì œ&ì§€ë¬¸ ì´ë¯¸ì§€ ì¶œë ¥**\n"
      ],
      "metadata": {
        "id": "ERJ8-0NBrnf7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import re\n",
        "import unicodedata\n",
        "from IPython.display import Image, display\n",
        "from IPython.display import Image, display\n",
        "\n",
        "# ì¶œì²˜ ì½”ë“œ ì¶”ì¶œ í•¨ìˆ˜\n",
        "def extract_question_code(source_str):\n",
        "    if not source_str:\n",
        "        return None\n",
        "    source_str = unicodedata.normalize('NFC', source_str)\n",
        "    source_str = source_str.replace(\"á„€á…®á†¨á„‹á…¥\", \"êµ­ì–´\").replace(\"á„‰á…®á„‚á…³á†¼\", \"ìˆ˜ëŠ¥\")\n",
        "\n",
        "    # ì¶œì²˜ ì½”ë“œ ì •ê·œì‹ ì¶”ì¶œ (ëì— ìˆëŠ” ì½”ë“œ ìš°ì„ )\n",
        "    match = re.search(r'(\\d{4}-(?:\\d{2}|ìˆ˜ëŠ¥)-êµ­ì–´_\\d+)$', source_str)\n",
        "    if match:\n",
        "        return match.group(1)\n",
        "    # ì „ì²´ ì¤‘ ë§ˆì§€ë§‰ ì½”ë“œ ì¶”ì¶œ\n",
        "    all_matches = re.findall(r'(\\d{4}-(?:\\d{2}|ìˆ˜ëŠ¥)-êµ­ì–´_\\d+)', source_str)\n",
        "    return all_matches[-1] if all_matches else None\n",
        "\n",
        "# ì´ë¯¸ì§€ ì €ì¥ ë£¨íŠ¸ í´ë”\n",
        "IMAGE_ROOT = \"/content/drive/MyDrive/Colab Notebooks/TAVE í”„ë¡œì íŠ¸_STUBO/ë¬¸í•™/data/output_images\"\n",
        "\n",
        "# ë¬¸ì œ ì´ë¯¸ì§€ ê²½ë¡œ ë°˜í™˜\n",
        "def get_problem_image_path(question_code):\n",
        "    if not question_code:\n",
        "        return None\n",
        "    return os.path.join(IMAGE_ROOT, f\"{question_code}.png\")\n",
        "\n",
        "# ì§€ë¬¸ ì´ë¯¸ì§€ ê²½ë¡œ ë°˜í™˜\n",
        "def get_passage_image_path(passage_code):\n",
        "    if not passage_code:\n",
        "        return None\n",
        "    passage_code = unicodedata.normalize(\"NFC\", passage_code)\n",
        "    match = re.match(r\"(\\d{4}-(?:\\d{2}|ìˆ˜ëŠ¥)-êµ­ì–´)(_p\\d+)\", passage_code)\n",
        "    if not match:\n",
        "        print(f\"âŒ ì§€ë¬¸ ì½”ë“œ íŒŒì‹± ì‹¤íŒ¨: {passage_code}\")\n",
        "        return None\n",
        "    base, p_part = match.groups()\n",
        "    return os.path.join(IMAGE_ROOT, f\"{base}{p_part}.png\")\n",
        "\n",
        "# ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ì— íƒœê·¸ ë³‘í•©\n",
        "def merge_tags_to_docs(docs, tag_dict):\n",
        "    for doc in docs:\n",
        "        source = doc.metadata.get(\"ì¶œì²˜\") or doc.metadata.get(\"source\")\n",
        "        code = extract_question_code(source)\n",
        "        if not code:\n",
        "            print(f\"âŒ [ë³‘í•© ì‹¤íŒ¨] ì¶œì²˜ ì½”ë“œ ì¶”ì¶œ ì‹¤íŒ¨: {source}\")\n",
        "            continue\n",
        "        if code not in tag_dict:\n",
        "            print(f\"âŒ [ë³‘í•© ì‹¤íŒ¨] íƒœê·¸ ì—†ìŒ: {code}\")\n",
        "            continue\n",
        "        doc.metadata[\"question_tags\"] = tag_dict[code]\n",
        "        doc.metadata[\"ì¶œì²˜\"] = code\n",
        "    return docs"
      ],
      "metadata": {
        "id": "Oe4xSYS4fiGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tag_similarity_score(user_tags, doc_tags):\n",
        "    \"\"\"\n",
        "    ì‚¬ìš©ì íƒœê·¸(user_tags)ì™€ ë¬¸ì„œ íƒœê·¸(doc_tags)ë¥¼ ë¹„êµí•˜ì—¬ ìœ ì‚¬ë„ ì ìˆ˜ë¥¼ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.\n",
        "\n",
        "    ì ìˆ˜ ë°°ì :\n",
        "    - ë¬¸ì œ ìœ í˜•: 4ì  (ì™„ì „ ì¼ì¹˜ ì‹œ)\n",
        "    - ë³µí•©/ë‹¨ì¼: 2ì  (ì¼ì¹˜ ì‹œ)\n",
        "    - ì§€ë¬¸ ì¥ë¥´: 3ì  (ì™„ì „ ì¼ì¹˜), 2ì  (ìœ ì‚¬ ì¥ë¥´)\n",
        "    - ì§€ë¬¸ ì œëª©: 1ì  (í•˜ë‚˜ë¼ë„ ì¼ì¹˜ ì‹œ)\n",
        "    - ì§€ë¬¸ ì‘ê°€: 1ì  (í•˜ë‚˜ë¼ë„ ì¼ì¹˜ ì‹œ)\n",
        "    \"\"\"\n",
        "    score = 0\n",
        "\n",
        "    # 1) ë¬¸ì œ ìœ í˜• (4ì )\n",
        "    if user_tags.get(\"ë¬¸ì œ ìœ í˜•\") == doc_tags.get(\"ë¬¸ì œ ìœ í˜•\"):\n",
        "        score += 4\n",
        "\n",
        "    # 2) ë³µí•©/ë‹¨ì¼ (2ì )\n",
        "    if user_tags.get(\"ë³µí•©/ë‹¨ì¼\") == doc_tags.get(\"ë³µí•©/ë‹¨ì¼\"):\n",
        "        score += 2\n",
        "\n",
        "    # 3) ì§€ë¬¸ ì¥ë¥´ (5ì  ì™„ì „ ì¼ì¹˜, 2ì  ìœ ì‚¬ ì¥ë¥´)\n",
        "    user_genre = user_tags.get(\"ì§€ë¬¸ ì¥ë¥´\")\n",
        "    doc_genre = doc_tags.get(\"ì§€ë¬¸ ì¥ë¥´\")\n",
        "\n",
        "    if user_genre == doc_genre:\n",
        "        score += 3\n",
        "    else:\n",
        "        # ìœ ì‚¬ ì¥ë¥´ ìŒ (ë¶€ë¶„ ì ìˆ˜)\n",
        "        genre_similarities = [\n",
        "            (\"í˜„ëŒ€ì‹œ\", \"ê³ ì „ ì‹œê°€\"),\n",
        "            (\"ê³ ì „ ì‹œê°€\", \"í˜„ëŒ€ì‹œ\"),\n",
        "            (\"í˜„ëŒ€ ì†Œì„¤\", \"ê³ ì „ ì†Œì„¤\"),\n",
        "            (\"ê³ ì „ ì†Œì„¤\", \"í˜„ëŒ€ ì†Œì„¤\"),\n",
        "        ]\n",
        "        if (user_genre, doc_genre) in genre_similarities or (doc_genre, user_genre) in genre_similarities:\n",
        "            score += 2\n",
        "\n",
        "    # 4) ì§€ë¬¸ ì œëª© (1ì , í•˜ë‚˜ë¼ë„ ê²¹ì¹˜ë©´ ì ìˆ˜ ë¶€ì—¬)\n",
        "    user_title = user_tags.get(\"ì§€ë¬¸ ì œëª©\")\n",
        "    doc_title = doc_tags.get(\"ì§€ë¬¸ ì œëª©\")\n",
        "\n",
        "    if isinstance(user_title, list) and isinstance(doc_title, list):\n",
        "        if set(user_title) & set(doc_title):  # êµì§‘í•©ì´ ìˆìœ¼ë©´\n",
        "            score += 1\n",
        "    elif isinstance(user_title, list):\n",
        "        if doc_title in user_title:\n",
        "            score += 1\n",
        "    elif isinstance(doc_title, list):\n",
        "        if user_title in doc_title:\n",
        "            score += 1\n",
        "    else:\n",
        "        if user_title == doc_title:\n",
        "            score += 1\n",
        "\n",
        "    # 5) ì§€ë¬¸ ì‘ê°€ (1ì , í•˜ë‚˜ë¼ë„ ê²¹ì¹˜ë©´ ì ìˆ˜ ë¶€ì—¬)\n",
        "    user_name = user_tags.get(\"ì§€ë¬¸ ì‘ê°€\")\n",
        "    doc_name = doc_tags.get(\"ì§€ë¬¸ ì‘ê°€\")\n",
        "\n",
        "    if isinstance(user_name, list) and isinstance(doc_name, list):\n",
        "        if set(user_name) & set(doc_name):\n",
        "            score += 1\n",
        "    elif isinstance(user_name, list):\n",
        "        if doc_name in user_name:\n",
        "            score += 1\n",
        "    elif isinstance(doc_name, list):\n",
        "        if user_name in doc_name:\n",
        "            score += 1\n",
        "    else:\n",
        "        if user_name == doc_name:\n",
        "            score += 1\n",
        "\n",
        "    return score"
      ],
      "metadata": {
        "id": "wXOKfdZxd70-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_similar_problems_with_images(user_question, retriever, tag_dict, top_k=2):\n",
        "    print(\"\\nğŸ§  [ìœ ì‚¬ ë¬¸ì œ ì¶”ì²œ ì‹œì‘]\")\n",
        "    user_tags = get_tags_from_gpt(user_question)\n",
        "    if user_tags is None:\n",
        "        print(\"âŒ ì‚¬ìš©ì ì§ˆë¬¸ íƒœê¹… ì‹¤íŒ¨\")\n",
        "        return\n",
        "\n",
        "    print(\"\\n=== âœ… ì‚¬ìš©ì ì§ˆë¬¸ íƒœê·¸ ===\")\n",
        "    print(json.dumps(user_tags, ensure_ascii=False, indent=2))\n",
        "\n",
        "    results = retriever.similarity_search_with_score(user_question, k=30)\n",
        "\n",
        "    docs = []\n",
        "    for doc, score in results:\n",
        "        doc.metadata[\"score\"] = score  # â† ìœ ì‚¬ë„ ì ìˆ˜ë¥¼ metadataì— ë„£ì–´ì¤Œ\n",
        "        docs.append(doc)\n",
        "\n",
        "    docs = merge_tags_to_docs(docs, tag_dict)\n",
        "\n",
        "    docs_with_score = []\n",
        "    for doc in docs:\n",
        "        doc_tags = doc.metadata.get(\"question_tags\")\n",
        "        if not doc_tags:\n",
        "            continue\n",
        "        tag_sim = tag_similarity_score(user_tags, doc_tags)\n",
        "        embedding_sim = doc.metadata.get(\"score\", 0)\n",
        "        final_score = round(tag_sim * 0.7 + embedding_sim * 0.3, 4)\n",
        "        docs_with_score.append((doc, doc_tags, tag_sim, embedding_sim, final_score))\n",
        "\n",
        "    docs_sorted = sorted(docs_with_score, key=lambda x: x[4], reverse=True)\n",
        "\n",
        "    if not docs_sorted:\n",
        "        print(\"âŒ íƒœê·¸ ìˆëŠ” ìœ ì‚¬ ë¬¸í•­ ì—†ìŒ\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\n=== ğŸ” ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ìœ ì‚¬í•œ ê¸°ì¶œ ë¬¸ì œ Top {top_k} ===\")\n",
        "    for i, (doc, doc_tags, tag_sim, emb_sim, final_score) in enumerate(docs_sorted[:top_k]):\n",
        "        question_code = extract_question_code(doc.metadata.get(\"ì¶œì²˜\"))\n",
        "        passage_code = doc_tags.get(\"ì§€ë¬¸\")\n",
        "\n",
        "        if not question_code or not passage_code:\n",
        "            print(f\"âŒ ìœ ì‚¬ ë¬¸í•­ {i+1}ì˜ ë¬¸ì œ ì½”ë“œ ë˜ëŠ” ì§€ë¬¸ ì½”ë“œê°€ ì—†ìŠµë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.\")\n",
        "            continue\n",
        "\n",
        "        problem_img = get_problem_image_path(question_code)\n",
        "        passage_img = get_passage_image_path(passage_code)\n",
        "\n",
        "        print(f\"\\n[ìœ ì‚¬ ë¬¸ì œ {i + 1}]\")\n",
        "        print(f\"ğŸ“Œ ì¶œì²˜: {question_code}\")\n",
        "        print(f\"ğŸ§® íƒœê·¸ ìœ ì‚¬ë„: {tag_sim:.4f} | ì„ë² ë”© ìœ ì‚¬ë„: {emb_sim:.4f} | ìµœì¢… ì ìˆ˜: {final_score:.4f}\")\n",
        "\n",
        "        print(\"\\nğŸ·ï¸ íƒœê·¸ ì •ë³´:\")\n",
        "        print(json.dumps(doc_tags, ensure_ascii=False, indent=2))\n",
        "\n",
        "        print(\"\\nâ–¶ ë¬¸ì œ ì´ë¯¸ì§€:\")\n",
        "        if os.path.exists(problem_img):\n",
        "            display(Image(filename=problem_img))\n",
        "        else:\n",
        "            print(f\"âŒ ë¬¸ì œ ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤: {problem_img}\")\n",
        "\n",
        "        print(\"\\nâ–¶ ì§€ë¬¸ ì´ë¯¸ì§€:\")\n",
        "        if passage_img and os.path.exists(passage_img):\n",
        "            display(Image(filename=passage_img))\n",
        "        else:\n",
        "            print(f\"âŒ ì§€ë¬¸ ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤: {passage_img}\")"
      ],
      "metadata": {
        "id": "NheN-amYM1QH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# íƒœê·¸ ë¡œë“œ ë° ë”•ì…”ë„ˆë¦¬ ìƒì„±\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/TAVE í”„ë¡œì íŠ¸_STUBO/ë¬¸í•™/data/literature_tagged.json', 'r', encoding='utf-8') as f:\n",
        "    tag_list = json.load(f)\n",
        "\n",
        "tag_dict = {}\n",
        "for item in tag_list:\n",
        "    code = extract_question_code(item.get(\"ì¶œì²˜\"))\n",
        "    if code:\n",
        "        tag_dict[code] = {\n",
        "            \"ì¶œì²˜\": item.get(\"ì¶œì²˜\"),\n",
        "            \"ë¬¸ì œ ìœ í˜•\": item.get(\"ë¬¸ì œ ìœ í˜•\"),\n",
        "            \"ë³µí•©/ë‹¨ì¼\": item.get(\"ë³µí•©/ë‹¨ì¼\"),\n",
        "            \"ì§€ë¬¸ ì œëª©\": item.get(\"ì§€ë¬¸ ì œëª©\"),\n",
        "            \"ì§€ë¬¸ ì¥ë¥´\": item.get(\"ì§€ë¬¸ ì¥ë¥´\"),\n",
        "            \"ì§€ë¬¸ ì‘ê°€\": item.get(\"ì§€ë¬¸ ì‘ê°€\"),\n",
        "            \"ì§€ë¬¸\": item.get(\"ì§€ë¬¸\"),\n",
        "        }"
      ],
      "metadata": {
        "id": "ySCb2kbskLxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhkt0mXWELV8"
      },
      "source": [
        "# **4. ì´ë¯¸ì§€(ì§€ë¬¸(ì¡°ê¸ˆ ì„±ê³µ..?) / ë¬¸ì œ(ì„±ê³µ) ì¸ì‹**\n",
        "- ì—¬ê¸°ì„œëŠ” OCRë§Œ ì´ìš©í•´ì„œ í•´ì•¼í•¨ (ìˆ˜ì • í•„ìš”)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4a1G-36bXfE"
      },
      "source": [
        "### **<ì§€ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ>**\n",
        "ì´ë¯¸ì§€ 5ë“±ë¶„ -> gpt-4oë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ -> gpt-4oê°€ ì¶”ì¶œí•œ í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ë¥¼ ë¹„êµí•˜ë©´ íŠ¹ìˆ˜ë¬¸ì ì‚½ì… -> gpt-4oê°€ íŠ¹ìˆ˜ë¬¸ì ê²€í†  -> gpt-4oê°€ ì§€ë¬¸ ë²”ìœ„([A], [B] ë“±) ì‚½ì…"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-MqXc1UYbfq6"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import openai\n",
        "import base64\n",
        "import io\n",
        "from typing import List, Tuple\n",
        "\n",
        "# âœ… ì´ë¯¸ì§€ â†’ base64\n",
        "def image_to_base64(image: Image.Image) -> str:\n",
        "    buffer = io.BytesIO()\n",
        "    image.save(buffer, format=\"PNG\")\n",
        "    return f\"data:image/png;base64,{base64.b64encode(buffer.getvalue()).decode()}\"\n",
        "\n",
        "# âœ… ì´ë¯¸ì§€ ìˆ˜ì§ ë¶„í• \n",
        "def split_image_vertically(image: Image.Image, parts: int = 5) -> List[Image.Image]:\n",
        "    width, height = image.size\n",
        "    part_height = height // parts\n",
        "    return [\n",
        "        image.crop((0, i * part_height, width, height if i == parts - 1 else (i + 1) * part_height))\n",
        "        for i in range(parts)\n",
        "    ]\n",
        "\n",
        "# âœ… GPTì—ê²Œ OCR ì‹œí‚¤ëŠ” í•¨ìˆ˜\n",
        "def gpt_ocr_text(image: Image.Image) -> str:\n",
        "    base64_img = image_to_base64(image)\n",
        "\n",
        "    system_prompt = \"\"\"\n",
        "    ë„ˆëŠ” ìˆ˜ëŠ¥ êµ­ì–´ ë¬¸í•™ì˜ ì§€ë¬¸ OCR í…ìŠ¤íŠ¸ ì¶”ì¶œ ì „ë¬¸ê°€ì•¼.\n",
        "\n",
        "    ì•„ë˜ ì´ë¯¸ì§€ë¥¼ ë³´ê³  **OCR í…ìŠ¤íŠ¸ë¥¼ ê·¸ëŒ€ë¡œ ì¶”ì¶œ**í•´.\n",
        "    í…ìŠ¤íŠ¸ ì¶”ì¶œë§Œ í•˜ê³ , ì ˆëŒ€ ê°€ê³µí•˜ê±°ë‚˜ ì„¤ëª…í•˜ì§€ ë§ˆ.\n",
        "\n",
        "    ğŸ“Œ ë°˜ë“œì‹œ ì§€í‚¬ ê²ƒ:\n",
        "    - ì¤„ë°”ê¿ˆì€ ì´ë¯¸ì§€ì— ë³´ì´ëŠ” ê·¸ëŒ€ë¡œ ì‚´ë ¤ì•¼ í•´.\n",
        "    - ë„ì–´ì“°ê¸°, íŠ¹ìˆ˜ë¬¸ì, ê´„í˜¸, ë§ˆì¹¨í‘œ ë“± ëª¨ë“  ë¬¸ì¥ ë¶€í˜¸ë„ ê·¸ëŒ€ë¡œ ìœ ì§€í•´ì•¼ í•´.\n",
        "    - í•´ì„ì´ë‚˜ ë¶€ì—° ì„¤ëª… ì—†ì´ **ìˆœìˆ˜í•œ OCR ê²°ê³¼ë§Œ ì¶œë ¥**í•´ì•¼ í•´.\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        response = openai.chat.completions.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_prompt.strip()},\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": [\n",
        "                        {\"type\": \"image_url\", \"image_url\": {\n",
        "                            \"url\": base64_img,\n",
        "                            \"detail\": \"high\"\n",
        "                        }}\n",
        "                    ]\n",
        "                }\n",
        "            ],\n",
        "            temperature=0,\n",
        "            max_tokens=16000\n",
        "        )\n",
        "        output = response.choices[0].message.content.strip()\n",
        "        if not output or \"ì£„ì†¡í•˜ì§€ë§Œ\" in output:\n",
        "            raise ValueError(\"GPT OCR ì‹¤íŒ¨ ë˜ëŠ” ê²°ê³¼ ì—†ìŒ\")\n",
        "        return output\n",
        "    except Exception as e:\n",
        "        return f\"[âŒ GPT OCR ì‹¤íŒ¨]: {str(e)}\"\n",
        "\n",
        "# âœ… GPT-4oë¡œ íŠ¹ìˆ˜ê¸°í˜¸&ê´„í˜¸ ì‚½ì…\n",
        "def refine_text_with_gpt(image: Image.Image, ocr_output: str) -> str:\n",
        "    base64_img = image_to_base64(image)\n",
        "\n",
        "    system_prompt = \"\"\"\n",
        "    ë„ˆëŠ” ìˆ˜ëŠ¥ êµ­ì–´ ë¬¸í•™ ì§€ë¬¸ ì •ë¦¬ ì „ë¬¸ê°€ì•¼.\n",
        "\n",
        "    ë‹¤ìŒì€ OCRë¡œ ì¶”ì¶œí•œ ì§€ë¬¸ê³¼ ì›ë³¸ ì´ë¯¸ì§€ì•¼. ë„ˆëŠ” ì´ ë‘ ì •ë³´ë¥¼ ë¹„êµí•´ì„œ ë‹¤ìŒê³¼ ê°™ì´ OCR ê²°ê³¼ë¥¼ ì •í™•í•˜ê²Œ ìˆ˜ì •í•´ì•¼ í•´.\n",
        "\n",
        "    ğŸ“Œ ë°˜ë“œì‹œ ì§€ì¼œì•¼ í•  ê·œì¹™:\n",
        "\n",
        "    1. âœ… ì´ë¯¸ì§€ ì•ˆì—ì„œ íŠ¹ìˆ˜ê¸°í˜¸(ì›í˜• ë¬¸ì/ì•ŒíŒŒë²³)ë¥¼ ì •í™•íˆ ì‹ë³„í•˜ê³ , OCR í…ìŠ¤íŠ¸ì˜ ì•Œë§ì€ ìœ„ì¹˜ì— ì‚½ì…í•´ì•¼ í•´.\n",
        "      - OCR ê²°ê³¼ë§Œ ë³´ë©´ ì•ˆ ë¼. ë°˜ë“œì‹œ ì´ë¯¸ì§€ì—ì„œ íŠ¹ìˆ˜ê¸°í˜¸ ìœ„ì¹˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ íŒë‹¨í•´ì•¼ í•´.\n",
        "\n",
        "    2. âœ… ì‚½ì…í•´ì•¼ í•  íŠ¹ìˆ˜ê¸°í˜¸ëŠ” ë‹¤ìŒê³¼ ê°™ì•„:\n",
        "      - í•œê¸€ ì›í˜• ë¬¸ì: ã‰ , ã‰¡, ã‰¢, ã‰£, ã‰¤\n",
        "      - ì›í˜• ì•ŒíŒŒë²³: â“, â“‘, â“’, â““, â“”\n",
        "\n",
        "    3. âœ… **ëª¨ë“  íŠ¹ìˆ˜ê¸°í˜¸ ë’¤ì—ëŠ” ë°‘ì¤„ë¡œ ê°•ì¡°ëœ ë¬¸ì¥ì´ ë°˜ë“œì‹œ ìˆê³ **, ê·¸ **ê°•ì¡°ëœ ë¬¸ì¥ ì „ì²´ë¥¼ ê´„í˜¸ '( )'ë¡œ ë°˜ë“œì‹œ ê°ì‹¸ì•¼ í•´.**\n",
        "      - íŠ¹ìˆ˜ê¸°í˜¸ì™€ ê´„í˜¸ëŠ” ë¶™ì—¬ì„œ ì‘ì„±í•´: ì˜ˆ) â“(ê°•ì¡°ëœ ë¬¸ì¥)\n",
        "      - ê´„í˜¸ëŠ” í•´ë‹¹ ë¬¸ì¥ì˜ ì‹œì‘ê³¼ ëì„ ì •í™•íˆ ê°ì‹¸ì•¼ í•˜ë©°, ì¤„ë°”ê¿ˆì´ë‚˜ ê³µë°±ì´ ìˆì–´ë„ ì „ì²´ë¥¼ í¬í•¨í•´ì•¼ í•´.\n",
        "\n",
        "    4. âœ… í•œ íŠ¹ìˆ˜ê¸°í˜¸ì— í•´ë‹¹í•˜ëŠ” ê°•ì¡° ë¬¸ì¥ì´ **ì—¬ëŸ¬ ì¤„ì— ê±¸ì³ ìˆë”ë¼ë„** ê´„í˜¸ë¡œ ì •í™•íˆ ê°ì‹¸ì•¼ í•´.\n",
        "      - ì¤‘ê°„ ì¤„ë°”ê¿ˆì´ë‚˜ ê³µë°±ì´ ìˆë”ë¼ë„ ê°•ì¡° ë¬¸ì¥ ì „ì²´ê°€ ê´„í˜¸ ì•ˆì— ë“¤ì–´ê°€ì•¼ í•´.\n",
        "\n",
        "    5. âŒ íŠ¹ìˆ˜ê¸°í˜¸ê°€ ì—†ëŠ” ë¬¸ì¥ì€ ìˆ˜ì •í•˜ì§€ ë§ˆ.\n",
        "      âŒ ì² ì ì˜¤ë¥˜, ë„ì–´ì“°ê¸° ì˜¤ë¥˜ ë“± OCR ìì²´ ì˜¤ë¥˜ë„ ê³ ì¹˜ì§€ ë§ˆ.\n",
        "\n",
        "    6. âœ… ì¶œë ¥ì€ ë°˜ë“œì‹œ ì§€ë¬¸ ì „ì²´ë¥¼ í¬í•¨í•´ì•¼ í•˜ë©°, íŠ¹ìˆ˜ê¸°í˜¸ + ê°•ì¡°ë¬¸ì¥ ë¶€ë¶„ë§Œ ìˆ˜ì •í•´ì•¼ í•´.\n",
        "      âŒ ì„¤ëª…, í•´ì„¤, ì¶”ê°€ ì •ë³´ ì—†ì´ **ì§€ë¬¸ ì „ì²´ë§Œ ì¶œë ¥**í•´ì•¼ í•´.\n",
        "\n",
        "\n",
        "     âš ï¸ ë°˜ë“œì‹œ íŠ¹ìˆ˜ê¸°í˜¸ ë’¤ì—ëŠ” ë°‘ì¤„ë¡œ ê°•ì¡°ëœ ë¶€ë¶„ì´ ê´„í˜¸'()'ë¡œ ê°ì‹¸ì ¸ ìˆì–´ì•¼í•´.\n",
        "     âš ï¸ íŠ¹ìˆ˜ê¸°í˜¸ ë’¤ì— ê´„í˜¸ê°€ ì—†ë‹¤ë©´ ë‹¤ì‹œ ë°‘ì¤„ë¡œ ê°•ì¡°ëœ ë¶€ë¶„ì„ ë‹¤ì‹œ ì°¾ê³  ê´„í˜¸ë¥¼ ì¶”ê°€í•´ì¤˜.\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        response = openai.chat.completions.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_prompt.strip()},\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": [\n",
        "                        {\"type\": \"text\", \"text\": f\"OCR ì¶”ì¶œ ê²°ê³¼:\\n{ocr_output.strip()}\"},\n",
        "                        {\"type\": \"image_url\", \"image_url\": {\n",
        "                            \"url\": base64_img,\n",
        "                            \"detail\": \"auto\"\n",
        "                        }}\n",
        "                    ]\n",
        "                }\n",
        "            ],\n",
        "            temperature=0.1,\n",
        "            max_tokens=16000  # ê°€ëŠ¥í•œ ìµœëŒ€ê°’ ì‚¬ìš©\n",
        "        )\n",
        "        output = response.choices[0].message.content.strip()\n",
        "        if not output or \"ì£„ì†¡í•˜ì§€ë§Œ\" in output or \"ë„ì™€ë“œë¦´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤\" in output:\n",
        "            raise ValueError(\"GPT ì‘ë‹µ ì˜¤ë¥˜ ë˜ëŠ” ë‚´ìš© ì—†ìŒ\")\n",
        "        return output\n",
        "    except Exception as e:\n",
        "        return f\"[âŒ GPT ì •êµí™” ì‹¤íŒ¨]: {str(e)}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JWyPv8MJ1T5I"
      },
      "outputs": [],
      "source": [
        "# âœ… íŠ¹ìˆ˜ê¸°í˜¸ ìœ„ì¹˜ ì ê²€ ë° ë³´ì • ìš”ì²­\n",
        "def verify_special_symbols(original_image: Image.Image, restored_text: str) -> str:\n",
        "    base64_img = image_to_base64(original_image)\n",
        "\n",
        "    system_prompt = \"\"\"\n",
        "      ë„ˆëŠ” ìˆ˜ëŠ¥ êµ­ì–´ ë¬¸í•™ì˜ ì§€ë¬¸ OCR ë³µì› ì „ë¬¸ê°€ì•¼.\n",
        "\n",
        "      ë‹¤ìŒì€ OCRë¡œ ì¶”ì¶œí•œ ì§€ë¬¸ê³¼ ì›ë³¸ ì´ë¯¸ì§€ì•¼. ë„ˆëŠ” ì´ ë‘ ì •ë³´ë¥¼ ë¹„êµí•´ì„œ ë‹¤ìŒê³¼ ê°™ì´ OCR ê²°ê³¼ë¥¼ ì •í™•í•˜ê²Œ ìˆ˜ì •í•´ì•¼ í•´.\n",
        "      ì¶”ì¶œí•œ ì§€ë¬¸ì˜ íŠ¹ìˆ˜ê¸°í˜¸(ã‰ , ã‰¡, ã‰¢, ã‰£, ã‰¤, â“, â“‘, â“’, â““, â“” ë“±)ê°€ ì›ë³¸ ì´ë¯¸ì§€ì˜ íŠ¹ìˆ˜ê¸°í˜¸ì™€ ë‹¤ë¥¼ ìˆ˜ ìˆì–´ì„œ ìˆ˜ì •í•´ì¤˜.\n",
        "\n",
        "      ì›ë³¸ ì´ë¯¸ì§€ë¥¼ ë³´ê³ \n",
        "      - ë¬¸ì¥ ì• ê¸°í˜¸ê°€ ì´ë¯¸ì§€ì™€ ì •í™•íˆ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸í•˜ê³ ,\n",
        "      - ì˜ëª»ëœ ê¸°í˜¸ëŠ” ì˜¬ë°”ë¥¸ íŠ¹ìˆ˜ê¸°í˜¸ë¡œ ìˆ˜ì •í•´.\n",
        "      - ì´ë¯¸ì§€ì—ëŠ” íŠ¹ìˆ˜ê¸°í˜¸ê°€ ì—†ëŠ”ë° í…ìŠ¤íŠ¸ì—ëŠ” ìˆëŠ” ê²½ìš°ëŠ” íŠ¹ìˆ˜ê¸°í˜¸ë¥¼ ì‚­ì œí•´ì¤˜.\n",
        "      - **ì¶”ì¶œí•œ ì§€ë¬¸ì˜ íŠ¹ìˆ˜ê¸°í˜¸ ë’¤ì— ê´„í˜¸'('ê°€ ì—†ë‹¤ë©´ ì´ë¯¸ì§€ì—ì„œ ë°‘ì¤„ë¡œ ê°•ì¡°ëœ ë¶€ë¶„ì„ ì°¾ì•„ì„œ ê´„í˜¸ë¡œ ê°ì‹¸ì„œ ì§€ë¬¸ì„ ìˆ˜ì •í•´ì¤˜.**\n",
        "\n",
        "      âš ï¸ â‘ , â‘¡, â‘¢, â‘£, â‘¤ ì´ëŸ° ì›í˜• ìˆ«ì íŠ¹ìˆ˜ê¸°í˜¸ëŠ” ì§€ë¬¸ì— ìˆì„ ìˆ˜ ì—†ì–´ ë°˜ë“œì‹œ ë‹¤ì‹œ ì˜¬ë°”ë¥¸ íŠ¹ìˆ˜ê¸°í˜¸ë¡œ ìˆ˜ì •í•´ì¤˜.\n",
        "      âš ï¸ ë™ì¼í•œ íŠ¹ìˆ˜ê¸°í˜¸ê°€ ë˜ ë‚˜ì˜¬ ìˆ˜ ì—†ì–´ ë‹¤ì‹œ ì˜¬ë°”ë¥¸ íŠ¹ìˆ˜ê¸°í˜¸ë¡œ ìˆ˜ì •í•´ì¤˜.\n",
        "      âš ï¸ ë°˜ë“œì‹œ íŠ¹ìˆ˜ê¸°í˜¸ ë’¤ì—ëŠ” ë°‘ì¤„ë¡œ ê°•ì¡°ëœ ë¶€ë¶„ì´ ê´„í˜¸'()'ë¡œ ë°˜ë“œì‹œ ê°ì‹¸ì ¸ ìˆì–´ì•¼í•´.\n",
        "      âš ï¸ ì¶”ì¶œí•œ ì§€ë¬¸ì˜ íŠ¹ìˆ˜ê¸°í˜¸ ë’¤ì— ê´„í˜¸ê°€ ì—†ë‹¤ë©´ ì›ë³¸ ì´ë¯¸ì§€ì—ì„œ íŠ¹ìˆ˜ê¸°í˜¸ ë’¤ì— ë‹¤ì‹œ ë°‘ì¤„ë¡œ ê°•ì¡°ëœ ë¶€ë¶„ì„ ë‹¤ì‹œ ì°¾ê³  ì§€ë¬¸ í…ìŠ¤íŠ¸ì— ê´„í˜¸'()'ë¥¼ ì¶”ê°€í•´ì¤˜.\n",
        "      âš ï¸ ì´ë¯¸ì§€ ê¸°í˜¸ ìœ„ì¹˜ë¥¼ ê¼­ í™•ì¸í•´ íŒë‹¨í•˜ê³ ,\n",
        "      âš ï¸ ì¶œë ¥ì€ ë°˜ë“œì‹œ ì§€ë¬¸ ì „ì²´ë¥¼ ì¶œë ¥í•´.\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        response = openai.chat.completions.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_prompt.strip()},\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": [\n",
        "                        {\"type\": \"text\", \"text\": f\"ë³µì›ëœ ì§€ë¬¸ ì¼ë¶€:\\n{restored_text.strip()}\"},\n",
        "                        {\"type\": \"image_url\", \"image_url\": {\n",
        "                            \"url\": base64_img,\n",
        "                            \"detail\": \"high\"\n",
        "                        }}\n",
        "                    ]\n",
        "                }\n",
        "            ],\n",
        "            temperature=0.1,\n",
        "            max_tokens=16000\n",
        "        )\n",
        "        return response.choices[0].message.content.strip()\n",
        "    except Exception as e:\n",
        "        return f\"[âŒ GPT ê¸°í˜¸ ê²€í†  ì‹¤íŒ¨]: {str(e)}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iv2z1cW2FvKP"
      },
      "outputs": [],
      "source": [
        "# ì´ë¯¸ì§€ ìš°ì¸¡ ì§€ë¬¸ ë²”ìœ„ í‘œì‹œ\n",
        "def insert_passage_brackets_with_gpt(image: Image.Image, ocr_text: str) -> str:\n",
        "    base64_img = image_to_base64(image)\n",
        "\n",
        "    system_prompt = \"\"\"\n",
        "    ë„ˆëŠ” ìˆ˜ëŠ¥ êµ­ì–´ ë¬¸í•™ì˜ ì§€ë¬¸ OCR ë³µì› ì „ë¬¸ê°€ì•¼.\n",
        "\n",
        "    ë‹¤ìŒì€ OCRë¡œ ì¶”ì¶œí•œ ì§€ë¬¸ê³¼ ì›ë³¸ ì´ë¯¸ì§€ì•¼.\n",
        "    ì›ë³¸ ì´ë¯¸ì§€ì˜ **ì¢Œì¸¡ í˜¹ì€ ìš°ì¸¡ì— [A], [B], [C] ë“±ì˜ í‘œì‹œì™€ í•¨ê»˜ íŠ¹ì • ì§€ë¬¸ ë²”ìœ„ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì„ ë“¤ì´ ìˆëŠ” ê²½ìš°ê°€ ìˆì–´.**\n",
        "    ì´ ì‹œê°ì  ì •ë³´ëŠ” ì–´ë–¤ ì§€ë¬¸ êµ¬ê°„ì´ ë¬¸ì œ í’€ì´ì—ì„œ ì¤‘ìš”í•œì§€ë¥¼ ì•Œë ¤ì£¼ëŠ” ë‹¨ì„œì•¼.\n",
        "\n",
        "    ğŸ” ë„ˆì˜ ì„ë¬´ëŠ” ë‹¤ìŒê³¼ ê°™ì•„:\n",
        "\n",
        "    1. ì§€ë¬¸ ì´ë¯¸ì§€ì˜ ë§¨ ì™¼ìª½ê³¼ ë§¨ ì˜¤ë¥¸ìª½ì„ ì˜ ì‚´í´ë³´ê³ ,\n",
        "      íŠ¹ì • ì§€ë¬¸ ë²”ìœ„ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì„ ê³¼ í•¨ê»˜ [A], [B], [C] ë“±ì´ ì§€ë¬¸ ë‚´ **ì–´ë””ì„œë¶€í„° ì–´ë””ê¹Œì§€ë¥¼ ê°€ë¦¬í‚¤ëŠ”ì§€** íŒë‹¨í•´.\n",
        "        - ê° ë²”ìœ„ëŠ” ë°˜ë“œì‹œ **1ì¤„ ì´ìƒ**ì´ ë˜ë„ë¡ í•˜ê³ , **ë¬¸ì¥ì´ ëŠê¸°ì§€ ì•Šê²Œ ìì—°ìŠ¤ëŸ½ê²Œ í¬í•¨**í•´ì•¼ í•´.\n",
        "        - ë¬¸ì¥ì˜ ì‹œì‘ì´ë‚˜ ëì´ ì˜ë¦¬ì§€ ì•Šë„ë¡, ì§€ë¬¸ íë¦„ì— ë§ê²Œ í•´ë‹¹ **ì‹œì‘ ì¤„ê³¼ ë ì¤„ ì „ì²´ë¥¼ í¬í•¨**í•´ì•¼ í•´.\n",
        "\n",
        "    2. ë§Œì•½ íŠ¹ì • ì§€ë¬¸ ë²”ìœ„ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì„ ì´ ìˆë‹¤ë©´, íŒë‹¨ëœ ë²”ìœ„ë¥¼ ì§€ë¬¸ ì¤‘ ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ë˜ ì•„ë˜ ì˜ˆì‹œì²˜ëŸ¼ `[A]{}`ë¡œ ê°ì‹¸ ê°•ì¡°í•´ì¤˜:\n",
        "        ...\n",
        "        [A] {\n",
        "            (í•´ë‹¹ ë²”ìœ„ ì‹œì‘ ì¤„)\n",
        "            ...\n",
        "            (í•´ë‹¹ ë²”ìœ„ ë ì¤„)\n",
        "        }\n",
        "        ...\n",
        "\n",
        "    âš ï¸ ìœ ì˜ì‚¬í•­:\n",
        "    - ì¤„ ë‹¨ìœ„ë¡œ íŒë‹¨í•˜ë˜, ì˜ë¯¸ ë‹¨ìœ„(ë¬¸ì¥ êµ¬ì¡°)ë¥¼ ìµœëŒ€í•œ ìœ ì§€í•´ì•¼ í•´.\n",
        "    - íŠ¹ì • ì§€ë¬¸ ë²”ìœ„ì˜ ì‹œì‘ê³¼ ë ìœ„ì¹˜ëŠ” ì´ë¯¸ì§€ ì˜¤ë¥¸ìª½ì˜ ì„ ê³¼ ì‹œê°ì ìœ¼ë¡œ ì •ë ¬ëœ ì§€ë¬¸ ì¤„ì„ ì°¾ì•„ íŒë‹¨í•´ì•¼ í•´.\n",
        "    - ì¤„ë°”ê¿ˆ, ê³µë°±, ê´„í˜¸, íŠ¹ìˆ˜ê¸°í˜¸ ë“±ì€ OCR í…ìŠ¤íŠ¸ ìŠ¤íƒ€ì¼ì„ ê·¸ëŒ€ë¡œ ìœ ì§€í•´.\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        response = openai.chat.completions.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_prompt.strip()},\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": [\n",
        "                        {\"type\": \"text\", \"text\": f\"OCR ê²°ê³¼:\\n{ocr_text.strip()}\"},\n",
        "                        {\"type\": \"image_url\", \"image_url\": {\n",
        "                            \"url\": base64_img,\n",
        "                            \"detail\": \"high\"\n",
        "                        }}\n",
        "                    ]\n",
        "                }\n",
        "            ],\n",
        "            temperature=0.1,\n",
        "            max_tokens=16000\n",
        "        )\n",
        "        return response.choices[0].message.content.strip()\n",
        "    except Exception as e:\n",
        "        return f\"[âŒ GPT ì§€ë¬¸ ë²”ìœ„ ì‚½ì… ì‹¤íŒ¨]: {str(e)}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q4VPhEUEb5Ki"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "def run_split_pipeline(image_path: str, parts: int = 4):\n",
        "    print(\"ğŸ“‚ ì´ë¯¸ì§€ ë¡œë”© ì¤‘...\")\n",
        "    image = Image.open(image_path)\n",
        "\n",
        "    # 1. ì´ë¯¸ì§€ ìˆ˜ì§ ë¶„í• \n",
        "    print(f\"ğŸ”€ ì´ë¯¸ì§€ {parts}ë“±ë¶„ ì¤‘...\")\n",
        "    split_images = split_image_vertically(image, parts=parts)\n",
        "\n",
        "    # 2. ê° ë¶„í•  ì´ë¯¸ì§€ OCR\n",
        "    ocr_parts = []\n",
        "    for idx, part_img in enumerate(split_images):\n",
        "        print(f\"ğŸ” [{idx + 1}/{parts}] ë¶„í•  ì´ë¯¸ì§€ OCR ì¤‘...\")\n",
        "        part_text = gpt_ocr_text(part_img)\n",
        "        ocr_parts.append(part_text)\n",
        "\n",
        "    # 3. OCR ê²°ê³¼ ë³‘í•©\n",
        "    combined_ocr = \"\\n\".join(ocr_parts).strip()\n",
        "\n",
        "    # 4. GPTë¡œ ì •êµí™”(íŠ¹ìˆ˜ê¸°í˜¸&ê´„í˜¸ ì‚½ì…)\n",
        "    print(\"ğŸ”§ GPT ì •êµí™” ë‹¨ê³„ ì§„í–‰ ì¤‘...\")\n",
        "    refined_text = refine_text_with_gpt(image, combined_ocr)\n",
        "\n",
        "    # 5. íŠ¹ìˆ˜ê¸°í˜¸ ìœ„ì¹˜ ê²€í†  ë° ë³´ì • => ì„±ê³µ\n",
        "    print(\"ğŸ§  íŠ¹ìˆ˜ê¸°í˜¸ ìœ„ì¹˜ ê²€í†  ë° ë³´ì • ì¤‘...\")\n",
        "    symbol_corrected_text = verify_special_symbols(image, refined_text)\n",
        "\n",
        "    # 6. ì´ë¯¸ì§€ ìš°ì¸¡ ì§€ë¬¸ ë²”ìœ„ í‘œì‹œ\n",
        "    print(\"ğŸ—‚ï¸ GPT ì§€ë¬¸ ë²”ìœ„ í‘œì‹œ([A], [B] ë“±) ì‚½ì… ì¤‘...\")\n",
        "    final_result = insert_passage_brackets_with_gpt(image, symbol_corrected_text)\n",
        "\n",
        "    return final_result.strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itQlrdn8GuoV"
      },
      "source": [
        "### **<ë¬¸ì œ í…ìŠ¤íŠ¸ ì¶”ì¶œ>**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nFPHxj62GyBd"
      },
      "outputs": [],
      "source": [
        "def extract_question(question_path):\n",
        "    import mimetypes\n",
        "\n",
        "    # íŒŒì¼ í™•ì¥ìì— ë§ê²Œ MIME íƒ€ì… ì¶”ì •\n",
        "    mime_type, _ = mimetypes.guess_type(question_path)\n",
        "    if not mime_type:\n",
        "        mime_type = \"image/png\"  # ê¸°ë³¸ê°’ fallback\n",
        "\n",
        "    with open(question_path, \"rb\") as f:\n",
        "        base64_img = base64.b64encode(f.read()).decode(\"utf-8\")\n",
        "\n",
        "    image_url = f\"data:{mime_type};base64,{base64_img}\"\n",
        "\n",
        "    system_prompt = \"\"\"\n",
        "    ë„ˆëŠ” ìˆ˜ëŠ¥ êµ­ì–´ ë¬¸í•™ ë¬¸ì œ ì´ë¯¸ì§€ë¥¼ í…ìŠ¤íŠ¸ë¡œ ì •í™•í•˜ê²Œ ë³µì›í•˜ëŠ” OCR ëª¨ë¸ì´ì•¼.\n",
        "\n",
        "    ì•„ë˜ ì´ë¯¸ì§€ëŠ” ë¬¸í•™ ë¬¸ì œ í•˜ë‚˜ì˜ 'ì§ˆë¬¸ ë¬¸ì¥ + â‘ ~â‘¤ ì„ íƒì§€'ê°€ í¬í•¨ëœ ì´ë¯¸ì§€ì•¼.\n",
        "    ë§Œì•½ <ë³´ê¸°> ë¬¸ì¥ì´ ì¡´ì¬í•œë‹¤ë©´ ì§ˆë¬¸ ì•ì— ìœ„ì¹˜í•˜ë©°, ë°˜ë“œì‹œ í¬í•¨í•´ì„œ ì¶œë ¥í•´.\n",
        "\n",
        "    í˜•ì‹ì€ ì•„ë˜ì™€ ê°™ì´ ì¶œë ¥í•´:\n",
        "\n",
        "    (ì§ˆë¬¸ê³¼ <ë³´ê¸°> ë‚´ìš©. <ë³´ê¸°>ê°€ ì—†ë‹¤ë©´ ìƒëµ)\n",
        "    â‘  ...\n",
        "    â‘¡ ...\n",
        "    â‘¢ ...\n",
        "    â‘£ ...\n",
        "    â‘¤ ...\n",
        "\n",
        "    â— ì ˆëŒ€ ì„¤ëª…ì´ë‚˜ ë¶€ê°€ í…ìŠ¤íŠ¸ë¥¼ ì¶”ê°€í•˜ì§€ ë§ê³  í˜•ì‹ ê·¸ëŒ€ë¡œ ì¶œë ¥í•´.\n",
        "    \"\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt.strip()},\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\n",
        "                        \"type\": \"image_url\",\n",
        "                        \"image_url\": {\n",
        "                            \"url\": image_url\n",
        "                        }\n",
        "                    }\n",
        "                ]\n",
        "            }\n",
        "        ],\n",
        "        max_tokens=1500,\n",
        "        temperature=0.1\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content.strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. í…ŒìŠ¤íŠ¸**"
      ],
      "metadata": {
        "id": "9NR7HUd23zm0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import unicodedata\n",
        "import os\n",
        "import re\n",
        "\n",
        "def parse_range(filename):\n",
        "    match = re.search(r'(\\d+)(?:~(\\d+))?', filename)\n",
        "    if match:\n",
        "        start = int(match.group(1))\n",
        "        end = int(match.group(2)) if match.group(2) else start\n",
        "        return list(range(start, end + 1))\n",
        "    return []\n",
        "\n",
        "def is_passage_file(filename):\n",
        "    return bool(re.match(r'.*ì§€ë¬¸.*\\d+(~\\d+)?\\.(png|PNG)$', filename))\n",
        "\n",
        "def is_question_file(filename, num):\n",
        "    pattern = rf'.*ë¬¸ì œ.*_{num}\\.(png|PNG)$'\n",
        "    return re.match(pattern, filename, re.IGNORECASE)\n",
        "\n",
        "def normalize_filenames(filenames):\n",
        "    return [unicodedata.normalize('NFC', f) for f in filenames]\n",
        "\n",
        "def auto_test_passage_then_questions(base_path, retriever, tag_dict, top_k=2):\n",
        "    files = os.listdir(base_path)\n",
        "    files = normalize_filenames(files)  # ì—¬ê¸°ì„œ ì •ê·œí™” ì ìš©\n",
        "\n",
        "    passage_files = sorted(\n",
        "        [f for f in files if is_passage_file(f)],\n",
        "        key=extract_index\n",
        "    )\n",
        "\n",
        "    print(f\"[DEBUG] ì§€ë¬¸ íŒŒì¼ ëª©ë¡: {passage_files}\")\n",
        "\n",
        "    if not passage_files:\n",
        "        print(\"âŒ ì§€ë¬¸ íŒŒì¼ì´ í•˜ë‚˜ë„ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "        return\n",
        "\n",
        "    for pf in passage_files:\n",
        "        problem_nums = parse_range(pf)\n",
        "        print(f\"[DEBUG] íŒŒì¼: {pf} â†’ ë¬¸ì œ ë²ˆí˜¸: {problem_nums}\")\n",
        "        if not problem_nums:\n",
        "            print(f\"\\nâŒ ë²ˆí˜¸ íŒŒì‹± ì‹¤íŒ¨: {pf}\")\n",
        "            continue\n",
        "\n",
        "        passage_path = os.path.join(base_path, pf)\n",
        "        try:\n",
        "            passage_text = run_split_pipeline(passage_path)\n",
        "            print(f\"[DEBUG] ì§€ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì„±ê³µ: {pf} (ê¸¸ì´: {len(passage_text)})\")\n",
        "        except Exception as e:\n",
        "            print(f\"\\nâŒ ì§€ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {pf} â†’ {e}\")\n",
        "            continue\n",
        "\n",
        "        print(f\"\\nğŸ“˜ ì§€ë¬¸: {pf} â†’ í¬í•¨ ë¬¸ì œ ë²ˆí˜¸: {problem_nums}\")\n",
        "        print(\"=\" * 40)\n",
        "\n",
        "        for num in problem_nums:\n",
        "            question_path = None\n",
        "            for f in files:\n",
        "                if is_question_file(f, num):\n",
        "                    question_path = os.path.join(base_path, f)\n",
        "                    break\n",
        "\n",
        "            print(f\"[DEBUG] ë¬¸ì œ {num} ì´ë¯¸ì§€ ê²½ë¡œ: {question_path}\")\n",
        "            if not question_path:\n",
        "                print(f\"âš ï¸ ë¬¸ì œ ì´ë¯¸ì§€ ì—†ìŒ: ë¬¸ì œ ë²ˆí˜¸ {num}\")\n",
        "                continue\n",
        "\n",
        "            print(f\"\\n--- â“ ë¬¸ì œ {num} ---\")\n",
        "            try:\n",
        "                question_text = extract_question(question_path)\n",
        "                print(f\"[DEBUG] ë¬¸ì œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì„±ê³µ: ë¬¸ì œ {num} (ê¸¸ì´: {len(question_text)})\")\n",
        "            except Exception as e:\n",
        "                print(f\"âŒ ë¬¸ì œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {question_path} â†’ {e}\")\n",
        "                continue\n",
        "\n",
        "            combined_text = f\"{passage_text.strip()}\\n\\n{question_text.strip()}\"\n",
        "\n",
        "            show_similar_problems_with_images(\n",
        "                combined_text,\n",
        "                retriever,\n",
        "                tag_dict,\n",
        "                top_k=top_k\n",
        "            )\n",
        "\n",
        "    print(\"\\nâœ… ì „ì²´ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!\")"
      ],
      "metadata": {
        "id": "CE8xdqvlZBmh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_path = \"/content/drive/MyDrive/Colab Notebooks/TAVE í”„ë¡œì íŠ¸_STUBO/ë¬¸í•™/data/ìˆ˜íŠ¹\"\n",
        "auto_test_passage_then_questions(base_path, retriever_store, tag_dict, top_k=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TkcMnHWZg_TX",
        "outputId": "dbe118c8-cb82-41b5-e7fd-f4d4ba2752ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DEBUG] ì§€ë¬¸ íŒŒì¼ ëª©ë¡: ['ìˆ˜íŠ¹_ë¬¸í•™_ì§€ë¬¸_1~2.PNG', 'ìˆ˜íŠ¹_ë¬¸í•™_ì§€ë¬¸_3.png', 'ìˆ˜íŠ¹_ë¬¸í•™_ì§€ë¬¸_4~5.png', 'ìˆ˜íŠ¹_ë¬¸í•™_ì§€ë¬¸_6.PNG', 'ìˆ˜íŠ¹_ë¬¸í•™_ì§€ë¬¸_7~9.PNG', 'ìˆ˜íŠ¹_ë¬¸í•™_ì§€ë¬¸_10~12.PNG', 'ìˆ˜íŠ¹_ë¬¸í•™_ì§€ë¬¸_13~15.PNG', 'ìˆ˜íŠ¹_ë¬¸í•™_ì§€ë¬¸_16~18.png', 'ìˆ˜íŠ¹_ë¬¸í•™_ì§€ë¬¸_19~21.png', 'ìˆ˜íŠ¹_ë¬¸í•™_ì§€ë¬¸_22~27.png', 'ìˆ˜íŠ¹_ë¬¸í•™_ì§€ë¬¸_29~30.png']\n",
            "[DEBUG] íŒŒì¼: ìˆ˜íŠ¹_ë¬¸í•™_ì§€ë¬¸_1~2.PNG â†’ ë¬¸ì œ ë²ˆí˜¸: [1, 2]\n",
            "ğŸ“‚ ì´ë¯¸ì§€ ë¡œë”© ì¤‘...\n",
            "\n",
            "âŒ ì§€ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: ìˆ˜íŠ¹_ë¬¸í•™_ì§€ë¬¸_1~2.PNG â†’ type object 'Image' has no attribute 'open'\n",
            "[DEBUG] íŒŒì¼: ìˆ˜íŠ¹_ë¬¸í•™_ì§€ë¬¸_3.png â†’ ë¬¸ì œ ë²ˆí˜¸: [3]\n",
            "ğŸ“‚ ì´ë¯¸ì§€ ë¡œë”© ì¤‘...\n",
            "\n",
            "âŒ ì§€ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: ìˆ˜íŠ¹_ë¬¸í•™_ì§€ë¬¸_3.png â†’ type object 'Image' has no attribute 'open'\n",
            "[DEBUG] íŒŒì¼: ìˆ˜íŠ¹_ë¬¸í•™_ì§€ë¬¸_4~5.png â†’ ë¬¸ì œ ë²ˆí˜¸: [4, 5]\n",
            "ğŸ“‚ ì´ë¯¸ì§€ ë¡œë”© ì¤‘...\n",
            "\n",
            "âŒ ì§€ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: ìˆ˜íŠ¹_ë¬¸í•™_ì§€ë¬¸_4~5.png â†’ type object 'Image' has no attribute 'open'\n",
            "[DEBUG] íŒŒì¼: ìˆ˜íŠ¹_ë¬¸í•™_ì§€ë¬¸_6.PNG â†’ ë¬¸ì œ ë²ˆí˜¸: [6]\n",
            "ğŸ“‚ ì´ë¯¸ì§€ ë¡œë”© ì¤‘...\n",
            "\n",
            "âŒ ì§€ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: ìˆ˜íŠ¹_ë¬¸í•™_ì§€ë¬¸_6.PNG â†’ type object 'Image' has no attribute 'open'\n",
            "[DEBUG] íŒŒì¼: ìˆ˜íŠ¹_ë¬¸í•™_ì§€ë¬¸_7~9.PNG â†’ ë¬¸ì œ ë²ˆí˜¸: [7, 8, 9]\n",
            "ğŸ“‚ ì´ë¯¸ì§€ ë¡œë”© ì¤‘...\n",
            "\n",
            "âŒ ì§€ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: ìˆ˜íŠ¹_ë¬¸í•™_ì§€ë¬¸_7~9.PNG â†’ type object 'Image' has no attribute 'open'\n",
            "[DEBUG] íŒŒì¼: ìˆ˜íŠ¹_ë¬¸í•™_ì§€ë¬¸_10~12.PNG â†’ ë¬¸ì œ ë²ˆí˜¸: [10, 11, 12]\n",
            "ğŸ“‚ ì´ë¯¸ì§€ ë¡œë”© ì¤‘...\n",
            "\n",
            "âŒ ì§€ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: ìˆ˜íŠ¹_ë¬¸í•™_ì§€ë¬¸_10~12.PNG â†’ type object 'Image' has no attribute 'open'\n",
            "[DEBUG] íŒŒì¼: ìˆ˜íŠ¹_ë¬¸í•™_ì§€ë¬¸_13~15.PNG â†’ ë¬¸ì œ ë²ˆí˜¸: [13, 14, 15]\n",
            "ğŸ“‚ ì´ë¯¸ì§€ ë¡œë”© ì¤‘...\n",
            "\n",
            "âŒ ì§€ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: ìˆ˜íŠ¹_ë¬¸í•™_ì§€ë¬¸_13~15.PNG â†’ type object 'Image' has no attribute 'open'\n",
            "[DEBUG] íŒŒì¼: ìˆ˜íŠ¹_ë¬¸í•™_ì§€ë¬¸_16~18.png â†’ ë¬¸ì œ ë²ˆí˜¸: [16, 17, 18]\n",
            "ğŸ“‚ ì´ë¯¸ì§€ ë¡œë”© ì¤‘...\n",
            "\n",
            "âŒ ì§€ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: ìˆ˜íŠ¹_ë¬¸í•™_ì§€ë¬¸_16~18.png â†’ type object 'Image' has no attribute 'open'\n",
            "[DEBUG] íŒŒì¼: ìˆ˜íŠ¹_ë¬¸í•™_ì§€ë¬¸_19~21.png â†’ ë¬¸ì œ ë²ˆí˜¸: [19, 20, 21]\n",
            "ğŸ“‚ ì´ë¯¸ì§€ ë¡œë”© ì¤‘...\n",
            "\n",
            "âŒ ì§€ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: ìˆ˜íŠ¹_ë¬¸í•™_ì§€ë¬¸_19~21.png â†’ type object 'Image' has no attribute 'open'\n",
            "[DEBUG] íŒŒì¼: ìˆ˜íŠ¹_ë¬¸í•™_ì§€ë¬¸_22~27.png â†’ ë¬¸ì œ ë²ˆí˜¸: [22, 23, 24, 25, 26, 27]\n",
            "ğŸ“‚ ì´ë¯¸ì§€ ë¡œë”© ì¤‘...\n",
            "\n",
            "âŒ ì§€ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: ìˆ˜íŠ¹_ë¬¸í•™_ì§€ë¬¸_22~27.png â†’ type object 'Image' has no attribute 'open'\n",
            "[DEBUG] íŒŒì¼: ìˆ˜íŠ¹_ë¬¸í•™_ì§€ë¬¸_29~30.png â†’ ë¬¸ì œ ë²ˆí˜¸: [29, 30]\n",
            "ğŸ“‚ ì´ë¯¸ì§€ ë¡œë”© ì¤‘...\n",
            "\n",
            "âŒ ì§€ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: ìˆ˜íŠ¹_ë¬¸í•™_ì§€ë¬¸_29~30.png â†’ type object 'Image' has no attribute 'open'\n",
            "\n",
            "âœ… ì „ì²´ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vAkYN8L9rBMY"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "KiOQqoaFr9xC",
        "-BC67ybd_0zG",
        "wDHgeg9sraka",
        "ERJ8-0NBrnf7",
        "hhkt0mXWELV8",
        "z4a1G-36bXfE",
        "itQlrdn8GuoV",
        "9NR7HUd23zm0"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}