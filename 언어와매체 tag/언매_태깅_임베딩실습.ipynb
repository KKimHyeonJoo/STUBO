{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 0. íŒ¨í‚¤ì§€ ì„¤ì¹˜ ë° í™˜ê²½ì„¤ì •, ëª¨ë“ˆ ë¡œë“œ"
      ],
      "metadata": {
        "id": "Yn_0cNDo011n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_75jZUVE0yew"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install \"unstructured[pdf]\""
      ],
      "metadata": {
        "id": "3Gf2Nqc1023X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y poppler-utils"
      ],
      "metadata": {
        "id": "KId0ZVtN065f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_huggingface"
      ],
      "metadata": {
        "id": "leOkWznS08Ls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -U langchain-community"
      ],
      "metadata": {
        "id": "P6f7jrb90-QH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu"
      ],
      "metadata": {
        "id": "qfTDtRo_0_kj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfplumber"
      ],
      "metadata": {
        "id": "AISXiQa06UOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unstructured.partition.pdf import partition_pdf\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.docstore.document import Document\n",
        "import numpy as np\n",
        "import pickle\n",
        "import os"
      ],
      "metadata": {
        "id": "PyImbQQF1AwM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = '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'"
      ],
      "metadata": {
        "id": "wN25hz1U1B7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfplumber"
      ],
      "metadata": {
        "id": "jU6tZJijd0Rs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "from langchain.schema import Document\n",
        "from langchain.vectorstores import FAISS\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch"
      ],
      "metadata": {
        "id": "zQu-IRqf1Dgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pdfplumber\n",
        "import re\n",
        "import json\n",
        "import os\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "LlVr5rN8d3s6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade openai"
      ],
      "metadata": {
        "id": "c7jnpR-Ad__p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI(api_key='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')"
      ],
      "metadata": {
        "id": "ZtnH9nkoeApJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. PDF ì—…ë¡œë“œ ë° í…ìŠ¤íŠ¸ ì¶”ì¶œ"
      ],
      "metadata": {
        "id": "Ez2Lbhge1GIz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "folder_path = \"/content/drive/MyDrive/á„‹á…¥á†«á„‹á…¥á„‹á…ªá„†á…¢á„á…¦_á„€á…µá„á…®á†¯á„†á…®á†«á„Œá…¦/\"\n",
        "\n",
        "file_list = [\n",
        "    os.path.join(folder_path, f)\n",
        "    for f in os.listdir(folder_path)\n",
        "    if os.path.isfile(os.path.join(folder_path, f))\n",
        "]\n",
        "\n",
        "print(\"ğŸ“ ì´ íŒŒì¼ ìˆ˜:\", len(file_list))\n",
        "for file in file_list:\n",
        "    print(file)"
      ],
      "metadata": {
        "id": "HtBrxg-l2Edu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. ê²½ë¡œ ì„¤ì •\n",
        "pdf_folder = \"/content/drive/MyDrive/á„‹á…¥á†«á„‹á…¥á„‹á…ªá„†á…¢á„á…¦_á„€á…µá„á…®á†¯á„†á…®á†«á„Œá…¦\"\n",
        "save_folder = \"/content/drive/MyDrive/á„‹á…¥á†«á„‹á…¥á„‹á…ªá„†á…¢á„á…¦_á„€á…µá„á…®á†¯á„†á…®á†«á„Œá…¦/parsed\"\n",
        "os.makedirs(save_folder, exist_ok=True)\n",
        "\n",
        "# 2. íŒŒì¼ ì²˜ë¦¬ ë£¨í”„\n",
        "pdf_files = sorted([f for f in os.listdir(pdf_folder) if f.endswith(\".pdf\")])\n",
        "\n",
        "pattern = re.compile(r\"(?P<id>\\d{1,2})\\.(?=\\s|[^0-9])\")  # ë¬¸ì œë²ˆí˜¸ íŒ¨í„´\n",
        "\n",
        "for filename in tqdm(pdf_files, desc=\"ğŸ“„ PDF ì²˜ë¦¬ ì¤‘\"):\n",
        "    try:\n",
        "        # 3. íŒŒì¼ëª…ì—ì„œ ì—°ë„, ì›” ì¶”ì¶œ: ì˜ˆ) \"2025_11.pdf\" â†’ 2025, 11\n",
        "        match = re.match(r\"(\\d{4})_(\\d{1,2})\\.pdf\", filename)\n",
        "        if not match:\n",
        "            print(f\"âŒ íŒŒì¼ëª… í˜•ì‹ ì˜¤ë¥˜: {filename}\")\n",
        "            continue\n",
        "\n",
        "        year, month = int(match.group(1)), int(match.group(2))\n",
        "        pdf_path = os.path.join(pdf_folder, filename)\n",
        "        save_path = os.path.join(save_folder, f\"{year}_{month}.jsonl\")\n",
        "\n",
        "        # 4. PDF ì—´ê¸° ë° í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
        "        with pdfplumber.open(pdf_path) as pdf:\n",
        "            full_text = \"\\n\".join(\n",
        "                page.extract_text() for page in pdf.pages if page.extract_text()\n",
        "            )\n",
        "\n",
        "        # 5. ë¬¸ì œë³„ ìë¥´ê¸°\n",
        "        matches = list(pattern.finditer(full_text))\n",
        "        questions = []\n",
        "        for i in range(len(matches)):\n",
        "            start = matches[i].start()\n",
        "            end = matches[i + 1].start() if i + 1 < len(matches) else len(full_text)\n",
        "            qid = int(matches[i].group(\"id\"))\n",
        "            qtext = full_text[start:end].strip()\n",
        "            questions.append({\n",
        "                \"id\": qid,\n",
        "                \"subject\": \"ì–¸ì–´ì™€ ë§¤ì²´\",\n",
        "                \"year\": year,\n",
        "                \"month\": month,\n",
        "                \"question\": qtext\n",
        "            })\n",
        "\n",
        "        # 6. ì €ì¥\n",
        "        with open(save_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            for q in questions:\n",
        "                f.write(json.dumps(q, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "        print(f\"âœ… {filename} â†’ {len(questions)}ë¬¸í•­ ì €ì¥ ì™„ë£Œ\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"ğŸ”¥ ì—ëŸ¬ ë°œìƒ: {filename} - {e}\")"
      ],
      "metadata": {
        "id": "4aiV4ZEM5tiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "# 1. ì €ì¥ëœ jsonl íŒŒì¼ì´ ìˆëŠ” í´ë” ê²½ë¡œ\n",
        "jsonl_folder = \"/content/drive/MyDrive/á„‹á…¥á†«á„‹á…¥á„‹á…ªá„†á…¢á„á…¦_á„€á…µá„á…®á†¯á„†á…®á†«á„Œá…¦/parsed\"\n",
        "\n",
        "# 2. íŒŒì¼ ëª©ë¡ ë¶ˆëŸ¬ì˜¤ê¸° (.jsonl í™•ì¥ìë§Œ)\n",
        "jsonl_files = sorted([\n",
        "    f for f in os.listdir(jsonl_folder)\n",
        "    if f.endswith(\".jsonl\")\n",
        "])\n",
        "\n",
        "# 3. ì „ì²´ ë¬¸í•­ ë¦¬ìŠ¤íŠ¸ì— ì €ì¥\n",
        "all_questions = []\n",
        "\n",
        "for filename in jsonl_files:\n",
        "    filepath = os.path.join(jsonl_folder, filename)\n",
        "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            try:\n",
        "                data = json.loads(line.strip())\n",
        "                all_questions.append(data)\n",
        "            except json.JSONDecodeError as e:\n",
        "                print(f\"âŒ JSON íŒŒì‹± ì—ëŸ¬: {filename} - {e}\")\n",
        "\n",
        "# 4. í™•ì¸\n",
        "print(f\"ğŸ“š ì´ ë¶ˆëŸ¬ì˜¨ ë¬¸í•­ ìˆ˜: {len(all_questions)}\")\n",
        "print(\"ì˜ˆì‹œ:\")\n",
        "for q in all_questions[:2]:\n",
        "    print(f\"- {q['year']}ë…„ {q['month']}ì›” [{q['id']}ë²ˆ]\\n  {q['question'][:100]}...\\n\")"
      ],
      "metadata": {
        "id": "N_ZxutN77sKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. íƒœê¹… ì„¤ì •"
      ],
      "metadata": {
        "id": "suA3ACuieK7B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# âœ… ê²½ë¡œ ì„¤ì •\n",
        "input_folder = \"/content/drive/MyDrive/á„‹á…¥á†«á„‹á…¥á„‹á…ªá„†á…¢á„á…¦_á„€á…µá„á…®á†¯á„†á…®á†«á„Œá…¦/parsed\"\n",
        "output_folder = \"/content/drive/MyDrive/á„‹á…¥á†«á„‹á…¥á„‹á…ªá„†á…¢á„á…¦_á„€á…µá„á…®á†¯á„†á…®á†«á„Œá…¦/tag\"\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "# âœ… GPT íƒœê·¸ ìƒì„± í•¨ìˆ˜\n",
        "def generate_tags_with_gpt(question_text):\n",
        "    prompt = (\n",
        "        \"ë‹¤ìŒì€ ê³ ë“±í•™êµ ìœ¤ë¦¬ ë¬¸ì œì´ë‹¤. ë¬¸ì œì˜ í•µì‹¬ ê°œë…, ë“±ì¥ ì‚¬ìƒê°€, ì‚¬ì¡°, ìœ¤ë¦¬ ì´ë¡  ë“±ì„ ë¶„ì„í•´ \"\n",
        "        \"ìµœëŒ€ 6ê°œì˜ í‚¤ì›Œë“œë¡œ ì¶”ì¶œí•´ì¤˜. í˜•ì‹ì€ JSON ë°°ì—´ë¡œë§Œ ì¶œë ¥í•´ì¤˜.\\n\\n\"\n",
        "        f\"ë¬¸ì œ:\\n{question_text}\\n\\ní‚¤ì›Œë“œ:\"\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4\",  # ë˜ëŠ” gpt-3.5-turbo\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=0.3,\n",
        "            max_tokens=150\n",
        "        )\n",
        "        reply = response.choices[0].message.content.strip()\n",
        "        tags = json.loads(reply)\n",
        "        if isinstance(tags, list):\n",
        "            return tags\n",
        "        else:\n",
        "            return [\"í˜•ì‹ ì˜¤ë¥˜\"]\n",
        "    except Exception as e:\n",
        "        return [f\"API ì˜¤ë¥˜: {str(e)}\"]\n",
        "\n",
        "# âœ… íŒŒì¼ë³„ ì²˜ë¦¬\n",
        "jsonl_files = sorted([f for f in os.listdir(input_folder) if f.endswith(\".jsonl\")])\n",
        "\n",
        "for filename in tqdm(jsonl_files, desc=\"ğŸ“‚ 35~45ë²ˆ ë¬¸ì œë§Œ ì²˜ë¦¬ ì¤‘\"):\n",
        "    input_path = os.path.join(input_folder, filename)\n",
        "    output_path = os.path.join(output_folder, filename.replace(\".jsonl\", \"_ì–¸ë§¤_tagged.jsonl\"))\n",
        "\n",
        "    # 1. ì „ì²´ ë¬¸í•­ ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "    questions = []\n",
        "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            questions.append(json.loads(line.strip()))\n",
        "\n",
        "    # 2. 35~45ë²ˆ í•„í„°ë§\n",
        "    selected = [q for q in questions if 35 <= q[\"id\"] <= 45]\n",
        "\n",
        "    # 3. íƒœê·¸ ìƒì„±\n",
        "    for q in selected:\n",
        "        q[\"tags\"] = generate_tags_with_gpt(q[\"question\"])\n",
        "\n",
        "    # 4. ì €ì¥ (35~45ë²ˆ ë¬¸í•­ë§Œ ì €ì¥)\n",
        "    with open(output_path, \"w\", encoding=\"utf-8\") as f_out:\n",
        "        for q in selected:\n",
        "            f_out.write(json.dumps(q, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "    print(f\"âœ… {filename} â†’ 35~45ë²ˆ ë¬¸ì œ ì €ì¥ ì™„ë£Œ: {output_path}\")"
      ],
      "metadata": {
        "id": "9frwEaEdBtyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. ì„ë² ë”©"
      ],
      "metadata": {
        "id": "6chx0lkIeO7Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import pickle\n",
        "import numpy as np\n",
        "import faiss\n",
        "from tqdm import tqdm\n",
        "from openai import OpenAI\n",
        "\n",
        "# âœ… OpenAI API í´ë¼ì´ì–¸íŠ¸\n",
        "client = OpenAI(api_key='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')\n",
        "\n",
        "# âœ… ê²½ë¡œ ì„¤ì •\n",
        "input_dir = \"/content/drive/MyDrive/á„‹á…¥á†«á„‹á…¥á„‹á…ªá„†á…¢á„á…¦_á„€á…µá„á…®á†¯á„†á…®á†«á„Œá…¦/tag\"\n",
        "output_dir = \"/content/drive/MyDrive/á„‹á…¥á†«á„‹á…¥á„‹á…ªá„†á…¢á„á…¦_á„€á…µá„á…®á†¯á„†á…®á†«á„Œá…¦/faiss_index\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# âœ… ì„ë² ë”© í•¨ìˆ˜\n",
        "def get_embeddings(batch_texts, model=\"text-embedding-3-small\"):\n",
        "    response = client.embeddings.create(\n",
        "        input=batch_texts,\n",
        "        model=model\n",
        "    )\n",
        "    return [d.embedding for d in response.data]\n",
        "\n",
        "# âœ… ë°°ì¹˜ ë¶„í•  í•¨ìˆ˜\n",
        "def batch(iterable, batch_size=100):\n",
        "    for i in range(0, len(iterable), batch_size):\n",
        "        yield iterable[i:i + batch_size]\n",
        "\n",
        "# âœ… ì—°ë„/ì›” ë¦¬ìŠ¤íŠ¸ ìƒì„±\n",
        "years = [2022, 2023, 2024, 2025]\n",
        "months = [3, 6, 9, 11]\n",
        "targets = [\n",
        "    f\"{year}_{month}_ì–¸ë§¤_tagged.jsonl\"\n",
        "    for year in years\n",
        "    for month in months\n",
        "    if not (year == 2022 and month < 3) and not (year == 2025 and month > 11)\n",
        "]\n",
        "\n",
        "# âœ… ì „ì²´ ë°˜ë³µ ì²˜ë¦¬\n",
        "for fname in tqdm(targets, desc=\"ğŸ“„ ì „ì²´ íŒŒì¼ ì²˜ë¦¬ ì¤‘\"):\n",
        "    try:\n",
        "        file_path = os.path.join(input_dir, fname)\n",
        "        base_name = fname.replace(\"_ì–¸ë§¤_tagged.jsonl\", \"\")  # ì˜ˆ: 2022_3\n",
        "\n",
        "        # íŒŒì¼ì´ ì—†ìœ¼ë©´ ê±´ë„ˆëœ€\n",
        "        if not os.path.exists(file_path):\n",
        "            print(f\"âŒ íŒŒì¼ ì—†ìŒ: {file_path}\")\n",
        "            continue\n",
        "\n",
        "        # 1. í…ìŠ¤íŠ¸ ë° ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘\n",
        "        texts = []\n",
        "        metadata = []\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            for line in f:\n",
        "                obj = json.loads(line)\n",
        "                if \"question\" in obj:\n",
        "                    texts.append(obj[\"question\"])\n",
        "                    metadata.append({\n",
        "                        \"id\": obj[\"id\"],\n",
        "                        \"year\": obj[\"year\"],\n",
        "                        \"month\": obj[\"month\"],\n",
        "                        \"subject\": obj[\"subject\"],\n",
        "                        \"tags\": obj.get(\"tags\", []),\n",
        "                        \"question\": obj[\"question\"]\n",
        "                    })\n",
        "\n",
        "        # ë¹„ì–´ìˆëŠ” ê²½ìš° ê±´ë„ˆëœ€\n",
        "        if len(texts) == 0:\n",
        "            print(f\"âš ï¸ ë¬¸í•­ ì—†ìŒ: {file_path}\")\n",
        "            continue\n",
        "\n",
        "        # 2. ì„ë² ë”© ìƒì„±\n",
        "        all_embeddings = []\n",
        "        for batch_texts in batch(texts, 100):\n",
        "            emb = get_embeddings(batch_texts)\n",
        "            all_embeddings.extend(emb)\n",
        "\n",
        "        # 3. FAISS ì¸ë±ìŠ¤ ìƒì„± ë° ì €ì¥\n",
        "        embedding_array = np.array(all_embeddings).astype(\"float32\")\n",
        "        dim = embedding_array.shape[1]\n",
        "        index = faiss.IndexFlatL2(dim)\n",
        "        index.add(embedding_array)\n",
        "\n",
        "        # 3. ì €ì¥ ê²½ë¡œ ì„¤ì •\n",
        "        faiss_path = os.path.join(output_dir, f\"text_index_{base_name}.faiss\")\n",
        "        meta_path = os.path.join(output_dir, f\"text_metadata_{base_name}.pkl\")\n",
        "\n",
        "        faiss.write_index(index, faiss_path)\n",
        "        with open(meta_path, \"wb\") as f:\n",
        "            pickle.dump(metadata, f)\n",
        "\n",
        "        print(f\"âœ… ì €ì¥ ì™„ë£Œ: {base_name}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ ì˜¤ë¥˜ ë°œìƒ ({fname}): {e}\")"
      ],
      "metadata": {
        "id": "ZKoXl6fVa8GP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}