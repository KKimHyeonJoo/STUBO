{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 0. Ìå®ÌÇ§ÏßÄ ÏÑ§Ïπò Î∞è ÌôòÍ≤ΩÏÑ§Ï†ï, Î™®Îìà Î°úÎìú"
      ],
      "metadata": {
        "id": "Yn_0cNDo011n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_75jZUVE0yew"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install \"unstructured[pdf]\""
      ],
      "metadata": {
        "id": "3Gf2Nqc1023X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y poppler-utils"
      ],
      "metadata": {
        "id": "KId0ZVtN065f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_huggingface"
      ],
      "metadata": {
        "id": "leOkWznS08Ls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -U langchain-community"
      ],
      "metadata": {
        "id": "P6f7jrb90-QH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu"
      ],
      "metadata": {
        "id": "qfTDtRo_0_kj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfplumber"
      ],
      "metadata": {
        "id": "AISXiQa06UOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unstructured.partition.pdf import partition_pdf\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.docstore.document import Document\n",
        "import numpy as np\n",
        "import pickle\n",
        "import os"
      ],
      "metadata": {
        "id": "PyImbQQF1AwM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = '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'"
      ],
      "metadata": {
        "id": "wN25hz1U1B7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfplumber"
      ],
      "metadata": {
        "id": "jU6tZJijd0Rs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "from langchain.schema import Document\n",
        "from langchain.vectorstores import FAISS\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch"
      ],
      "metadata": {
        "id": "zQu-IRqf1Dgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pdfplumber\n",
        "import re\n",
        "import json\n",
        "import os\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "LlVr5rN8d3s6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade openai"
      ],
      "metadata": {
        "id": "c7jnpR-Ad__p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI(api_key='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')"
      ],
      "metadata": {
        "id": "ZtnH9nkoeApJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. PDF ÏóÖÎ°úÎìú Î∞è ÌÖçÏä§Ìä∏ Ï∂îÏ∂ú"
      ],
      "metadata": {
        "id": "Ez2Lbhge1GIz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "folder_path = \"/content/drive/MyDrive/·Ñã·Ö•·Ü´·Ñã·Ö•·Ñã·Ö™·ÑÜ·Ö¢·Ñé·Ö¶_·ÑÄ·Öµ·Ñé·ÖÆ·ÜØ·ÑÜ·ÖÆ·Ü´·Ñå·Ö¶/\"\n",
        "\n",
        "file_list = [\n",
        "    os.path.join(folder_path, f)\n",
        "    for f in os.listdir(folder_path)\n",
        "    if os.path.isfile(os.path.join(folder_path, f))\n",
        "]\n",
        "\n",
        "print(\"üìÅ Ï¥ù ÌååÏùº Ïàò:\", len(file_list))\n",
        "for file in file_list:\n",
        "    print(file)"
      ],
      "metadata": {
        "id": "HtBrxg-l2Edu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Í≤ΩÎ°ú ÏÑ§Ï†ï\n",
        "pdf_folder = \"/content/drive/MyDrive/·Ñã·Ö•·Ü´·Ñã·Ö•·Ñã·Ö™·ÑÜ·Ö¢·Ñé·Ö¶_·ÑÄ·Öµ·Ñé·ÖÆ·ÜØ·ÑÜ·ÖÆ·Ü´·Ñå·Ö¶\"\n",
        "save_folder = \"/content/drive/MyDrive/·Ñã·Ö•·Ü´·Ñã·Ö•·Ñã·Ö™·ÑÜ·Ö¢·Ñé·Ö¶_·ÑÄ·Öµ·Ñé·ÖÆ·ÜØ·ÑÜ·ÖÆ·Ü´·Ñå·Ö¶/parsed\"\n",
        "os.makedirs(save_folder, exist_ok=True)\n",
        "\n",
        "# 2. ÌååÏùº Ï≤òÎ¶¨ Î£®ÌîÑ\n",
        "pdf_files = sorted([f for f in os.listdir(pdf_folder) if f.endswith(\".pdf\")])\n",
        "\n",
        "pattern = re.compile(r\"(?P<id>\\d{1,2})\\.(?=\\s|[^0-9])\")  # Î¨∏Ï†úÎ≤àÌò∏ Ìå®ÌÑ¥\n",
        "\n",
        "for filename in tqdm(pdf_files, desc=\"üìÑ PDF Ï≤òÎ¶¨ Ï§ë\"):\n",
        "    try:\n",
        "        # 3. ÌååÏùºÎ™ÖÏóêÏÑú Ïó∞ÎèÑ, Ïõî Ï∂îÏ∂ú: Ïòà) \"2025_11.pdf\" ‚Üí 2025, 11\n",
        "        match = re.match(r\"(\\d{4})_(\\d{1,2})\\.pdf\", filename)\n",
        "        if not match:\n",
        "            print(f\"‚ùå ÌååÏùºÎ™Ö ÌòïÏãù Ïò§Î•ò: {filename}\")\n",
        "            continue\n",
        "\n",
        "        year, month = int(match.group(1)), int(match.group(2))\n",
        "        pdf_path = os.path.join(pdf_folder, filename)\n",
        "        save_path = os.path.join(save_folder, f\"{year}_{month}.jsonl\")\n",
        "\n",
        "        # 4. PDF Ïó¥Í∏∞ Î∞è ÌÖçÏä§Ìä∏ Ï∂îÏ∂ú\n",
        "        with pdfplumber.open(pdf_path) as pdf:\n",
        "            full_text = \"\\n\".join(\n",
        "                page.extract_text() for page in pdf.pages if page.extract_text()\n",
        "            )\n",
        "\n",
        "        # 5. Î¨∏Ï†úÎ≥Ñ ÏûêÎ•¥Í∏∞\n",
        "        matches = list(pattern.finditer(full_text))\n",
        "        questions = []\n",
        "        for i in range(len(matches)):\n",
        "            start = matches[i].start()\n",
        "            end = matches[i + 1].start() if i + 1 < len(matches) else len(full_text)\n",
        "            qid = int(matches[i].group(\"id\"))\n",
        "            qtext = full_text[start:end].strip()\n",
        "            questions.append({\n",
        "                \"id\": qid,\n",
        "                \"subject\": \"Ïñ∏Ïñ¥ÏôÄ Îß§Ï≤¥\",\n",
        "                \"year\": year,\n",
        "                \"month\": month,\n",
        "                \"question\": qtext\n",
        "            })\n",
        "\n",
        "        # 6. Ï†ÄÏû•\n",
        "        with open(save_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            for q in questions:\n",
        "                f.write(json.dumps(q, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "        print(f\"‚úÖ {filename} ‚Üí {len(questions)}Î¨∏Ìï≠ Ï†ÄÏû• ÏôÑÎ£å\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"üî• ÏóêÎü¨ Î∞úÏÉù: {filename} - {e}\")"
      ],
      "metadata": {
        "id": "4aiV4ZEM5tiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "# 1. Ï†ÄÏû•Îêú jsonl ÌååÏùºÏù¥ ÏûàÎäî Ìè¥Îçî Í≤ΩÎ°ú\n",
        "jsonl_folder = \"/content/drive/MyDrive/·Ñã·Ö•·Ü´·Ñã·Ö•·Ñã·Ö™·ÑÜ·Ö¢·Ñé·Ö¶_·ÑÄ·Öµ·Ñé·ÖÆ·ÜØ·ÑÜ·ÖÆ·Ü´·Ñå·Ö¶/parsed\"\n",
        "\n",
        "# 2. ÌååÏùº Î™©Î°ù Î∂àÎü¨Ïò§Í∏∞ (.jsonl ÌôïÏû•ÏûêÎßå)\n",
        "jsonl_files = sorted([\n",
        "    f for f in os.listdir(jsonl_folder)\n",
        "    if f.endswith(\".jsonl\")\n",
        "])\n",
        "\n",
        "# 3. Ï†ÑÏ≤¥ Î¨∏Ìï≠ Î¶¨Ïä§Ìä∏Ïóê Ï†ÄÏû•\n",
        "all_questions = []\n",
        "\n",
        "for filename in jsonl_files:\n",
        "    filepath = os.path.join(jsonl_folder, filename)\n",
        "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            try:\n",
        "                data = json.loads(line.strip())\n",
        "                all_questions.append(data)\n",
        "            except json.JSONDecodeError as e:\n",
        "                print(f\"‚ùå JSON ÌååÏã± ÏóêÎü¨: {filename} - {e}\")\n",
        "\n",
        "# 4. ÌôïÏù∏\n",
        "print(f\"üìö Ï¥ù Î∂àÎü¨Ïò® Î¨∏Ìï≠ Ïàò: {len(all_questions)}\")\n",
        "print(\"ÏòàÏãú:\")\n",
        "for q in all_questions[:2]:\n",
        "    print(f\"- {q['year']}ÎÖÑ {q['month']}Ïõî [{q['id']}Î≤à]\\n  {q['question'][:100]}...\\n\")"
      ],
      "metadata": {
        "id": "N_ZxutN77sKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. ÌÉúÍπÖ ÏÑ§Ï†ï"
      ],
      "metadata": {
        "id": "suA3ACuieK7B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Í≤ΩÎ°ú ÏÑ§Ï†ï\n",
        "input_folder = \"/content/drive/MyDrive/·Ñã·Ö•·Ü´·Ñã·Ö•·Ñã·Ö™·ÑÜ·Ö¢·Ñé·Ö¶_·ÑÄ·Öµ·Ñé·ÖÆ·ÜØ·ÑÜ·ÖÆ·Ü´·Ñå·Ö¶/parsed\"\n",
        "output_folder = \"/content/drive/MyDrive/·Ñã·Ö•·Ü´·Ñã·Ö•·Ñã·Ö™·ÑÜ·Ö¢·Ñé·Ö¶_·ÑÄ·Öµ·Ñé·ÖÆ·ÜØ·ÑÜ·ÖÆ·Ü´·Ñå·Ö¶/tag\"\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "# ‚úÖ GPT ÌÉúÍ∑∏ ÏÉùÏÑ± Ìï®Ïàò\n",
        "def generate_tags_with_gpt(question_text):\n",
        "    prompt = (\n",
        "        \"Îã§ÏùåÏùÄ Í≥†Îì±ÌïôÍµê Ïú§Î¶¨ Î¨∏Ï†úÏù¥Îã§. Î¨∏Ï†úÏùò ÌïµÏã¨ Í∞úÎÖê, Îì±Ïû• ÏÇ¨ÏÉÅÍ∞Ä, ÏÇ¨Ï°∞, Ïú§Î¶¨ Ïù¥Î°† Îì±ÏùÑ Î∂ÑÏÑùÌï¥ \"\n",
        "        \"ÏµúÎåÄ 6Í∞úÏùò ÌÇ§ÏõåÎìúÎ°ú Ï∂îÏ∂úÌï¥Ï§ò. ÌòïÏãùÏùÄ JSON Î∞∞Ïó¥Î°úÎßå Ï∂úÎ†•Ìï¥Ï§ò.\\n\\n\"\n",
        "        f\"Î¨∏Ï†ú:\\n{question_text}\\n\\nÌÇ§ÏõåÎìú:\"\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4\",  # ÎòêÎäî gpt-3.5-turbo\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=0.3,\n",
        "            max_tokens=150\n",
        "        )\n",
        "        reply = response.choices[0].message.content.strip()\n",
        "        tags = json.loads(reply)\n",
        "        if isinstance(tags, list):\n",
        "            return tags\n",
        "        else:\n",
        "            return [\"ÌòïÏãù Ïò§Î•ò\"]\n",
        "    except Exception as e:\n",
        "        return [f\"API Ïò§Î•ò: {str(e)}\"]\n",
        "\n",
        "# ‚úÖ ÌååÏùºÎ≥Ñ Ï≤òÎ¶¨\n",
        "jsonl_files = sorted([f for f in os.listdir(input_folder) if f.endswith(\".jsonl\")])\n",
        "\n",
        "for filename in tqdm(jsonl_files, desc=\"üìÇ 35~45Î≤à Î¨∏Ï†úÎßå Ï≤òÎ¶¨ Ï§ë\"):\n",
        "    input_path = os.path.join(input_folder, filename)\n",
        "    output_path = os.path.join(output_folder, filename.replace(\".jsonl\", \"_Ïñ∏Îß§_tagged.jsonl\"))\n",
        "\n",
        "    # 1. Ï†ÑÏ≤¥ Î¨∏Ìï≠ Î∂àÎü¨Ïò§Í∏∞\n",
        "    questions = []\n",
        "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            questions.append(json.loads(line.strip()))\n",
        "\n",
        "    # 2. 35~45Î≤à ÌïÑÌÑ∞ÎßÅ\n",
        "    selected = [q for q in questions if 35 <= q[\"id\"] <= 45]\n",
        "\n",
        "    # 3. ÌÉúÍ∑∏ ÏÉùÏÑ±\n",
        "    for q in selected:\n",
        "        q[\"tags\"] = generate_tags_with_gpt(q[\"question\"])\n",
        "\n",
        "    # 4. Ï†ÄÏû• (35~45Î≤à Î¨∏Ìï≠Îßå Ï†ÄÏû•)\n",
        "    with open(output_path, \"w\", encoding=\"utf-8\") as f_out:\n",
        "        for q in selected:\n",
        "            f_out.write(json.dumps(q, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "    print(f\"‚úÖ {filename} ‚Üí 35~45Î≤à Î¨∏Ï†ú Ï†ÄÏû• ÏôÑÎ£å: {output_path}\")"
      ],
      "metadata": {
        "id": "9frwEaEdBtyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. ÏûÑÎ≤†Îî©"
      ],
      "metadata": {
        "id": "6chx0lkIeO7Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import pickle\n",
        "import numpy as np\n",
        "import faiss\n",
        "from tqdm import tqdm\n",
        "from openai import OpenAI\n",
        "\n",
        "# ‚úÖ OpenAI API ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏\n",
        "client = OpenAI(api_key='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')\n",
        "\n",
        "# ‚úÖ Í≤ΩÎ°ú ÏÑ§Ï†ï\n",
        "input_dir = \"/content/drive/MyDrive/·Ñã·Ö•·Ü´·Ñã·Ö•·Ñã·Ö™·ÑÜ·Ö¢·Ñé·Ö¶_·ÑÄ·Öµ·Ñé·ÖÆ·ÜØ·ÑÜ·ÖÆ·Ü´·Ñå·Ö¶/tag\"\n",
        "output_dir = \"/content/drive/MyDrive/·Ñã·Ö•·Ü´·Ñã·Ö•·Ñã·Ö™·ÑÜ·Ö¢·Ñé·Ö¶_·ÑÄ·Öµ·Ñé·ÖÆ·ÜØ·ÑÜ·ÖÆ·Ü´·Ñå·Ö¶/faiss_index\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# ‚úÖ ÏûÑÎ≤†Îî© Ìï®Ïàò\n",
        "def get_embeddings(batch_texts, model=\"text-embedding-3-small\"):\n",
        "    response = client.embeddings.create(\n",
        "        input=batch_texts,\n",
        "        model=model\n",
        "    )\n",
        "    return [d.embedding for d in response.data]\n",
        "\n",
        "# ‚úÖ Î∞∞Ïπò Î∂ÑÌï† Ìï®Ïàò\n",
        "def batch(iterable, batch_size=100):\n",
        "    for i in range(0, len(iterable), batch_size):\n",
        "        yield iterable[i:i + batch_size]\n",
        "\n",
        "# ‚úÖ Ïó∞ÎèÑ/Ïõî Î¶¨Ïä§Ìä∏ ÏÉùÏÑ±\n",
        "years = [2022, 2023, 2024, 2025]\n",
        "months = [3, 6, 9, 11]\n",
        "targets = [\n",
        "    f\"{year}_{month}_Ïñ∏Îß§_tagged.jsonl\"\n",
        "    for year in years\n",
        "    for month in months\n",
        "    if not (year == 2022 and month < 3) and not (year == 2025 and month > 11)\n",
        "]\n",
        "\n",
        "# ‚úÖ Ï†ÑÏ≤¥ Î∞òÎ≥µ Ï≤òÎ¶¨\n",
        "for fname in tqdm(targets, desc=\"üìÑ Ï†ÑÏ≤¥ ÌååÏùº Ï≤òÎ¶¨ Ï§ë\"):\n",
        "    try:\n",
        "        file_path = os.path.join(input_dir, fname)\n",
        "        base_name = fname.replace(\"_Ïñ∏Îß§_tagged.jsonl\", \"\")  # Ïòà: 2022_3\n",
        "\n",
        "        # ÌååÏùºÏù¥ ÏóÜÏúºÎ©¥ Í±¥ÎÑàÎúÄ\n",
        "        if not os.path.exists(file_path):\n",
        "            print(f\"‚ùå ÌååÏùº ÏóÜÏùå: {file_path}\")\n",
        "            continue\n",
        "\n",
        "        # 1. ÌÖçÏä§Ìä∏ Î∞è Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ ÏàòÏßë\n",
        "        texts = []\n",
        "        metadata = []\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            for line in f:\n",
        "                obj = json.loads(line)\n",
        "                if \"question\" in obj:\n",
        "                    texts.append(obj[\"question\"])\n",
        "                    metadata.append({\n",
        "                        \"id\": obj[\"id\"],\n",
        "                        \"year\": obj[\"year\"],\n",
        "                        \"month\": obj[\"month\"],\n",
        "                        \"subject\": obj[\"subject\"],\n",
        "                        \"tags\": obj.get(\"tags\", []),\n",
        "                        \"question\": obj[\"question\"]\n",
        "                    })\n",
        "\n",
        "        # ÎπÑÏñ¥ÏûàÎäî Í≤ΩÏö∞ Í±¥ÎÑàÎúÄ\n",
        "        if len(texts) == 0:\n",
        "            print(f\"‚ö†Ô∏è Î¨∏Ìï≠ ÏóÜÏùå: {file_path}\")\n",
        "            continue\n",
        "\n",
        "        # 2. ÏûÑÎ≤†Îî© ÏÉùÏÑ±\n",
        "        all_embeddings = []\n",
        "        for batch_texts in batch(texts, 100):\n",
        "            emb = get_embeddings(batch_texts)\n",
        "            all_embeddings.extend(emb)\n",
        "\n",
        "        # 3. FAISS Ïù∏Îç±Ïä§ ÏÉùÏÑ± Î∞è Ï†ÄÏû•\n",
        "        embedding_array = np.array(all_embeddings).astype(\"float32\")\n",
        "        dim = embedding_array.shape[1]\n",
        "        index = faiss.IndexFlatL2(dim)\n",
        "        index.add(embedding_array)\n",
        "\n",
        "        # 3. Ï†ÄÏû• Í≤ΩÎ°ú ÏÑ§Ï†ï\n",
        "        faiss_path = os.path.join(output_dir, f\"text_index_{base_name}.faiss\")\n",
        "        meta_path = os.path.join(output_dir, f\"text_metadata_{base_name}.pkl\")\n",
        "\n",
        "        faiss.write_index(index, faiss_path)\n",
        "        with open(meta_path, \"wb\") as f:\n",
        "            pickle.dump(metadata, f)\n",
        "\n",
        "        print(f\"‚úÖ Ï†ÄÏû• ÏôÑÎ£å: {base_name}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Ïò§Î•ò Î∞úÏÉù ({fname}): {e}\")"
      ],
      "metadata": {
        "id": "ZKoXl6fVa8GP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}